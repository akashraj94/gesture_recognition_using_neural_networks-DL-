{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSIQIiGiWHHK"
   },
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yn-buaLlWHHS",
    "outputId": "9d5dbd03-4d2d-406a-dbf7-39939d799483"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scipy==1.1.0\n",
      "  Downloading scipy-1.1.0-cp37-cp37m-manylinux1_x86_64.whl (31.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 31.2 MB 214 kB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scipy==1.1.0) (1.19.5)\n",
      "Installing collected packages: scipy\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.4.1\n",
      "    Uninstalling scipy-1.4.1:\n",
      "      Successfully uninstalled scipy-1.4.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pymc3 3.11.4 requires scipy>=1.2.0, but you have scipy 1.1.0 which is incompatible.\n",
      "plotnine 0.6.0 requires scipy>=1.2.0, but you have scipy 1.1.0 which is incompatible.\n",
      "jax 0.2.25 requires scipy>=1.2.1, but you have scipy 1.1.0 which is incompatible.\n",
      "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "Successfully installed scipy-1.1.0\n"
     ]
    }
   ],
   "source": [
    "pip install scipy==1.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "sT2g3WewWHHT"
   },
   "outputs": [],
   "source": [
    "### Importing Libraries\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread, imresize\n",
    "import datetime\n",
    "import abc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQtiaytSWHHV"
   },
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AfIwowINWHHV"
   },
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "#tf.set_random_seed(30)\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0QREaeuQYWDS",
    "outputId": "8993473f-f1cc-48f5-fe2b-a789911c2db9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "### Mounting drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uQ-MnumWHHW"
   },
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "I3TihpM6SsT4"
   },
   "outputs": [],
   "source": [
    "### Reading the data\n",
    "project_folder = '/content/gdrive/MyDrive/ML/Project_data'\n",
    "train_doc = np.random.permutation(open('/content/gdrive/MyDrive/ML/Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('/content/gdrive/MyDrive/ML/Project_data/val.csv').readlines())\n",
    "batch_size = 51"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2TLX025WHHX"
   },
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "E8C0DNQgmIrR"
   },
   "outputs": [],
   "source": [
    "# Parameters initialization\n",
    "nb_rows = 120   # X dimension of the image\n",
    "nb_cols = 120   # Y dimesnion of the image\n",
    "nb_frames = 30  # lenght of the video frames\n",
    "nb_channel = 3 # number of channels in images 3 for color(RGB) and 1 for Gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0g-fTAinmJJx"
   },
   "outputs": [],
   "source": [
    "# Helper function to generate a random affine transform on the iamge\n",
    "def get_random_affine():\n",
    "    dx, dy = np.random.randint(-1.7, 1.8, 2)\n",
    "    M = np.float32([[1, 0, dx], [0, 1, dy]])\n",
    "    return M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "b0bdF3MjtIlb"
   },
   "outputs": [],
   "source": [
    "class ModelBuilder(metaclass= abc.ABCMeta):\n",
    "    \n",
    "    def initialize_path(self,project_folder):\n",
    "        self.train_doc = train_doc\n",
    "        self.val_doc = val_doc\n",
    "        self.train_path = project_folder + '/' + 'train'\n",
    "        self.val_path =  project_folder + '/' + 'val'\n",
    "        self.num_train_sequences = len(self.train_doc)\n",
    "        self.num_val_sequences = len(self.val_doc)\n",
    "        \n",
    "    def initialize_image_properties(self,image_height=100,image_width=100):\n",
    "        self.image_height=image_height\n",
    "        self.image_width=image_width\n",
    "        self.channels=3\n",
    "        self.num_classes=5\n",
    "        self.total_frames=30\n",
    "          \n",
    "    def initialize_hyperparams(self,frames_to_sample=30,batch_size=20,num_epochs=20):\n",
    "        self.frames_to_sample=frames_to_sample\n",
    "        self.batch_size=batch_size\n",
    "        self.num_epochs=num_epochs\n",
    "        \n",
    "        \n",
    "    def generator(self,source_path, folder_list, augment=False):\n",
    "        img_idx = np.round(np.linspace(0,self.total_frames-1,self.frames_to_sample)).astype(int)\n",
    "        batch_size=self.batch_size\n",
    "        while True:\n",
    "            t = np.random.permutation(folder_list)\n",
    "            num_batches = len(t)//batch_size\n",
    "        \n",
    "            for batch in range(num_batches): \n",
    "                batch_data, batch_labels= self.init_batch_data(source_path,t,batch,batch_size,img_idx,augment)\n",
    "                yield batch_data, batch_labels \n",
    "\n",
    "            remaining_seq=len(t)%batch_size\n",
    "        \n",
    "            if (remaining_seq != 0):\n",
    "                batch_data, batch_labels= self.init_batch_data(source_path,t,num_batches,batch_size,img_idx,augment,remaining_seq)\n",
    "                yield batch_data, batch_labels \n",
    "    \n",
    "    \n",
    "    def init_batch_data(self,source_path,t,batch,batch_size,img_idx,augment,remaining_seq=0):\n",
    "    \n",
    "        seq_len = remaining_seq if remaining_seq else batch_size\n",
    "    \n",
    "        batch_data = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels)) \n",
    "        batch_labels = np.zeros((seq_len,self.num_classes)) \n",
    "    \n",
    "        if (augment): batch_data_aug = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels))\n",
    "\n",
    "        \n",
    "        for folder in range(seq_len): \n",
    "            imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) \n",
    "            for idx,item in enumerate(img_idx): \n",
    "                image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                image_resized=imresize(image,(self.image_height,self.image_width,3))\n",
    "            \n",
    "\n",
    "                batch_data[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n",
    "                batch_data[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n",
    "                batch_data[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n",
    "            \n",
    "                if (augment):\n",
    "                    shifted = cv2.warpAffine(image, \n",
    "                                             np.float32([[1, 0, np.random.randint(-30,30)],[0, 1, np.random.randint(-30,30)]]), \n",
    "                                            (image.shape[1], image.shape[0]))\n",
    "                    \n",
    "                    gray = cv2.cvtColor(shifted,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                    x0, y0 = np.argwhere(gray > 0).min(axis=0)\n",
    "                    x1, y1 = np.argwhere(gray > 0).max(axis=0) \n",
    "                    \n",
    "                    cropped=shifted[x0:x1,y0:y1,:]\n",
    "                    \n",
    "                    image_resized=imresize(cropped,(self.image_height,self.image_width,3))\n",
    "                    \n",
    "                    #shifted = cv2.warpAffine(image_resized, \n",
    "                    #                        np.float32([[1, 0, np.random.randint(-3,3)],[0, 1, np.random.randint(-3,3)]]), \n",
    "                    #                        (image_resized.shape[1], image_resized.shape[0]))\n",
    "            \n",
    "                    batch_data_aug[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n",
    "                    batch_data_aug[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n",
    "                    batch_data_aug[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n",
    "                \n",
    "            \n",
    "            batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            \n",
    "    \n",
    "        if (augment):\n",
    "            batch_data=np.concatenate([batch_data,batch_data_aug])\n",
    "            batch_labels=np.concatenate([batch_labels,batch_labels])\n",
    "\n",
    "        \n",
    "        return(batch_data,batch_labels)\n",
    "\n",
    "    #Let us create the train_generator and the val_generator which will be used in .fit_generator.\n",
    "\n",
    "    def train_model(self, model, augment_data=False):\n",
    "        train_generator = self.generator(self.train_path, self.train_doc,augment=augment_data)\n",
    "        val_generator = self.generator(self.val_path, self.val_doc)\n",
    "\n",
    "        model_name = 'model_init' + '_' + str(datetime.datetime.now()).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "        if not os.path.exists(model_name):\n",
    "            os.mkdir(model_name)\n",
    "        \n",
    "        filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "        LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)\n",
    "        callbacks_list = [checkpoint, LR]\n",
    "\n",
    "        if (self.num_train_sequences%self.batch_size) == 0:\n",
    "            steps_per_epoch = int(self.num_train_sequences/self.batch_size)\n",
    "        else:\n",
    "            steps_per_epoch = (self.num_train_sequences//self.batch_size) + 1\n",
    "\n",
    "        if (self.num_val_sequences%self.batch_size) == 0:\n",
    "            validation_steps = int(self.num_val_sequences/self.batch_size)\n",
    "        else:\n",
    "            validation_steps = (self.num_val_sequences//self.batch_size) + 1\n",
    "    \n",
    "        history=model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=self.num_epochs, verbose=1, \n",
    "                            callbacks=callbacks_list, validation_data=val_generator, \n",
    "                            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
    "        return history    \n",
    "\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def define_model(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7VeDi_yWHHa"
   },
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JrRJM-GWWHHa",
    "outputId": "312bbeba-130d-457d-b825-b11ffecb8478"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 10\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '/content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Project_data_1/train'\n",
    "val_path = '/content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Project_data_1/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 10# choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tz4EgmgWHHb"
   },
   "source": [
    "## Model\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "V4UhIDRQG7Bs"
   },
   "outputs": [],
   "source": [
    "#Importing Libs\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D, Conv2D, MaxPooling2D\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1AHP8iypjBK9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEvrT23VjFVh"
   },
   "source": [
    "### SAMPLE MODEL\n",
    "Let's train a model with just 1 epoch, 40 batch size, resolution 160x160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQpYJbOSWHHb"
   },
   "outputs": [],
   "source": [
    "\n",
    "#write your model here\n",
    "class ModelConv3D1(ModelBuilder):\n",
    "    \n",
    "    def define_model(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv3D(16, (3, 3, 3), padding='same',\n",
    "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        model.add(Dense(64,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "        model.add(Dense(self.num_classes,activation='softmax'))\n",
    "\n",
    "       #Now that you have written the model, the next step is to `compile` the model.\n",
    "       #When you print the `summary` of the model, you'll see the total number of parameters you have to train. \n",
    "\n",
    "        # optimiser = optimizers.Adam()\n",
    "        optimiser = 'sgd'\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UL5jdi_Ortwn",
    "outputId": "222bd2af-e905-4d8c-efd2-4be495925652"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d (Conv3D)             (None, 30, 160, 160, 16)  1312      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 30, 160, 160, 16)  0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 30, 160, 160, 16)  64       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling3d (MaxPooling3D  (None, 15, 80, 80, 16)   0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv3d_1 (Conv3D)           (None, 15, 80, 80, 32)    4128      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 15, 80, 80, 32)    0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 15, 80, 80, 32)   128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_1 (MaxPooling  (None, 7, 40, 40, 32)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_2 (Conv3D)           (None, 7, 40, 40, 64)     16448     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 7, 40, 40, 64)     0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 7, 40, 40, 64)    256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_2 (MaxPooling  (None, 3, 20, 20, 64)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_3 (Conv3D)           (None, 3, 20, 20, 128)    65664     \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 3, 20, 20, 128)    0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 3, 20, 20, 128)   512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_3 (MaxPooling  (None, 1, 10, 10, 128)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 12800)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               1638528   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,736,389\n",
      "Trainable params: 1,735,525\n",
      "Non-trainable params: 864\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_3d1=ModelConv3D1()\n",
    "conv_3d1.initialize_path(project_folder)\n",
    "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
    "conv_3d1.initialize_hyperparams(frames_to_sample=30,batch_size=40,num_epochs=1)\n",
    "conv_3d1_model=conv_3d1.define_model()\n",
    "conv_3d1_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNxSWco-W28D"
   },
   "source": [
    "## Model 1 - Base Model - No Data Augmentation Batch Size 40 and Epoch 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-4cbVzzW60P"
   },
   "outputs": [],
   "source": [
    "class ModelConv3D1(ModelBuilder):\n",
    "    \n",
    "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv3D(16, filtersize, padding='same',\n",
    "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(32, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(64, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(128, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "        model.add(Dense(self.num_classes,activation='softmax'))\n",
    "\n",
    "        optimiser = 'sgd'\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YaK0yapsXPGB",
    "outputId": "2b03fa4b-e0f3-4d84-8749-6485e75b1e15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d (Conv3D)             (None, 20, 160, 160, 16)  1312      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 20, 160, 160, 16)  0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 20, 160, 160, 16)  64       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling3d (MaxPooling3D  (None, 10, 80, 80, 16)   0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv3d_1 (Conv3D)           (None, 10, 80, 80, 32)    13856     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 10, 80, 80, 32)    0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 10, 80, 80, 32)   128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_1 (MaxPooling  (None, 5, 40, 40, 32)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_2 (Conv3D)           (None, 5, 40, 40, 64)     55360     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 5, 40, 40, 64)     0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 5, 40, 40, 64)    256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_2 (MaxPooling  (None, 2, 20, 20, 64)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_3 (Conv3D)           (None, 2, 20, 20, 128)    221312    \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 2, 20, 20, 128)    0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 2, 20, 20, 128)   512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_3 (MaxPooling  (None, 1, 10, 10, 128)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 12800)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                819264    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,117,061\n",
      "Trainable params: 1,116,325\n",
      "Non-trainable params: 736\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_3d1=ModelConv3D1()\n",
    "conv_3d1.initialize_path(project_folder)\n",
    "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
    "conv_3d1.initialize_hyperparams(frames_to_sample=20,batch_size=40,num_epochs=15)\n",
    "conv_3d1_model=conv_3d1.define_model()\n",
    "conv_3d1_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hjKSx2s7Xpsx",
    "outputId": "7945cd01-ff04-4b96-a494-c1cd689d72f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - ETA: 0s - loss: 0.2743 - categorical_accuracy: 0.9140  \n",
      "Epoch 00014: saving model to model_init_2022-01-2404_21_22.526406/model-00014-0.27426-0.91403-1.72316-0.36000.h5\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "17/17 [==============================] - 1480s 87s/step - loss: 0.2743 - categorical_accuracy: 0.9140 - val_loss: 1.7232 - val_categorical_accuracy: 0.3600 - lr: 0.0100\n",
      "Epoch 15/15\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2516 - categorical_accuracy: 0.9336  \n",
      "Epoch 00015: saving model to model_init_2022-01-2404_21_22.526406/model-00015-0.25163-0.93363-1.32615-0.47000.h5\n",
      "17/17 [==============================] - 1483s 87s/step - loss: 0.2516 - categorical_accuracy: 0.9336 - val_loss: 1.3262 - val_categorical_accuracy: 0.4700 - lr: 0.0020\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Params:\", conv_3d1_model.count_params())\n",
    "history_model1 = conv_3d1.train_model(conv_3d1_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrOqk_Xjjq7z"
   },
   "source": [
    "#### OBSERVATION:\n",
    "Model is overfitting, so trying with data augmentation and reducing filter size and resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G99eWmcHfAna"
   },
   "source": [
    "## Model 2\n",
    "\n",
    "Reducing filter size 2x2x2 and image resolution to 120x120."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14HlvxJ5d8pY"
   },
   "outputs": [],
   "source": [
    "class ModelConv3D3(ModelBuilder):\n",
    "    \n",
    "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv3D(16, filtersize, padding='same',\n",
    "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(32, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(64, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(128, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "        model.add(Dense(self.num_classes,activation='softmax'))\n",
    "\n",
    "        optimiser = 'sgd'\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-DYOn2xkfGJ7",
    "outputId": "ccfed801-c83f-44f5-c31e-07820758c486"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_4 (Conv3D)           (None, 16, 120, 120, 16)  400       \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 16, 120, 120, 16)  0         \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 16, 120, 120, 16)  64       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_4 (MaxPooling  (None, 8, 60, 60, 16)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_5 (Conv3D)           (None, 8, 60, 60, 32)     4128      \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 8, 60, 60, 32)     0         \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 8, 60, 60, 32)    128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_5 (MaxPooling  (None, 4, 30, 30, 32)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_6 (Conv3D)           (None, 4, 30, 30, 64)     16448     \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 4, 30, 30, 64)     0         \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 4, 30, 30, 64)    256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_6 (MaxPooling  (None, 2, 15, 15, 64)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_7 (Conv3D)           (None, 2, 15, 15, 128)    65664     \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 2, 15, 15, 128)    0         \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 2, 15, 15, 128)   512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_7 (MaxPooling  (None, 1, 7, 7, 128)     0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 6272)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               1605888   \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,762,613\n",
      "Trainable params: 1,761,109\n",
      "Non-trainable params: 1,504\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_3d3=ModelConv3D3()\n",
    "conv_3d3.initialize_path(project_folder)\n",
    "conv_3d3.initialize_image_properties(image_height=120,image_width=120)\n",
    "conv_3d3.initialize_hyperparams(frames_to_sample=16,batch_size=30,num_epochs=30)\n",
    "conv_3d3_model=conv_3d3.define_model(filtersize=(2,2,2),dense_neurons=256,dropout=0.5)\n",
    "conv_3d3_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hcEHG_PpfGn9",
    "outputId": "816ffb5c-9dbe-43e0-9bdb-c59cb132453b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 1762613\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:125: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: DeprecationWarning:     `imread` is deprecated!\n",
      "    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``imageio.imread`` instead.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:56: DeprecationWarning:     `imresize` is deprecated!\n",
      "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``skimage.transform.resize`` instead.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:75: DeprecationWarning:     `imresize` is deprecated!\n",
      "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 2.1438 - categorical_accuracy: 0.3281  \n",
      "Epoch 00001: saving model to model_init_2022-01-2517_36_10.362749/model-00001-2.14382-0.32805-1.64593-0.25000.h5\n",
      "23/23 [==============================] - 2785s 126s/step - loss: 2.1438 - categorical_accuracy: 0.3281 - val_loss: 1.6459 - val_categorical_accuracy: 0.2500 - lr: 0.0100\n",
      "Epoch 2/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.6370 - categorical_accuracy: 0.4510\n",
      "Epoch 00002: saving model to model_init_2022-01-2517_36_10.362749/model-00002-1.63699-0.45098-1.94472-0.15000.h5\n",
      "23/23 [==============================] - 96s 4s/step - loss: 1.6370 - categorical_accuracy: 0.4510 - val_loss: 1.9447 - val_categorical_accuracy: 0.1500 - lr: 0.0100\n",
      "Epoch 3/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.2998 - categorical_accuracy: 0.5219\n",
      "Epoch 00003: saving model to model_init_2022-01-2517_36_10.362749/model-00003-1.29984-0.52187-2.47167-0.18000.h5\n",
      "23/23 [==============================] - 95s 4s/step - loss: 1.2998 - categorical_accuracy: 0.5219 - val_loss: 2.4717 - val_categorical_accuracy: 0.1800 - lr: 0.0100\n",
      "Epoch 4/30\n",
      "22/23 [===========================>..] - ETA: 3s - loss: 1.1626 - categorical_accuracy: 0.5962\n",
      "Epoch 00004: saving model to model_init_2022-01-2517_36_10.362749/model-00004-1.16437-0.59502-2.90636-0.26000.h5\n",
      "23/23 [==============================] - 93s 4s/step - loss: 1.1644 - categorical_accuracy: 0.5950 - val_loss: 2.9064 - val_categorical_accuracy: 0.2600 - lr: 0.0100\n",
      "Epoch 5/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.9676 - categorical_accuracy: 0.6365\n",
      "Epoch 00005: saving model to model_init_2022-01-2517_36_10.362749/model-00005-0.96758-0.63650-3.86015-0.16000.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "23/23 [==============================] - 94s 4s/step - loss: 0.9676 - categorical_accuracy: 0.6365 - val_loss: 3.8602 - val_categorical_accuracy: 0.1600 - lr: 0.0100\n",
      "Epoch 6/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.0724 - categorical_accuracy: 0.5943\n",
      "Epoch 00006: saving model to model_init_2022-01-2517_36_10.362749/model-00006-1.07237-0.59427-4.20019-0.12000.h5\n",
      "23/23 [==============================] - 95s 4s/step - loss: 1.0724 - categorical_accuracy: 0.5943 - val_loss: 4.2002 - val_categorical_accuracy: 0.1200 - lr: 0.0020\n",
      "Epoch 7/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.9875 - categorical_accuracy: 0.6395\n",
      "Epoch 00007: saving model to model_init_2022-01-2517_36_10.362749/model-00007-0.98755-0.63952-4.43739-0.17000.h5\n",
      "23/23 [==============================] - 103s 5s/step - loss: 0.9875 - categorical_accuracy: 0.6395 - val_loss: 4.4374 - val_categorical_accuracy: 0.1700 - lr: 0.0020\n",
      "Epoch 8/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.9336 - categorical_accuracy: 0.6599\n",
      "Epoch 00008: saving model to model_init_2022-01-2517_36_10.362749/model-00008-0.93363-0.65988-4.21397-0.19000.h5\n",
      "23/23 [==============================] - 92s 4s/step - loss: 0.9336 - categorical_accuracy: 0.6599 - val_loss: 4.2140 - val_categorical_accuracy: 0.1900 - lr: 0.0020\n",
      "Epoch 9/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.8810 - categorical_accuracy: 0.6742\n",
      "Epoch 00009: saving model to model_init_2022-01-2517_36_10.362749/model-00009-0.88098-0.67421-4.51076-0.20000.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "23/23 [==============================] - 93s 4s/step - loss: 0.8810 - categorical_accuracy: 0.6742 - val_loss: 4.5108 - val_categorical_accuracy: 0.2000 - lr: 0.0020\n",
      "Epoch 10/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.8635 - categorical_accuracy: 0.6870\n",
      "Epoch 00010: saving model to model_init_2022-01-2517_36_10.362749/model-00010-0.86350-0.68703-4.26954-0.25000.h5\n",
      "23/23 [==============================] - 97s 4s/step - loss: 0.8635 - categorical_accuracy: 0.6870 - val_loss: 4.2695 - val_categorical_accuracy: 0.2500 - lr: 4.0000e-04\n",
      "Epoch 11/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.8634 - categorical_accuracy: 0.6840\n",
      "Epoch 00011: saving model to model_init_2022-01-2517_36_10.362749/model-00011-0.86344-0.68401-4.54010-0.23000.h5\n",
      "23/23 [==============================] - 94s 4s/step - loss: 0.8634 - categorical_accuracy: 0.6840 - val_loss: 4.5401 - val_categorical_accuracy: 0.2300 - lr: 4.0000e-04\n",
      "Epoch 12/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.8101 - categorical_accuracy: 0.6923\n",
      "Epoch 00012: saving model to model_init_2022-01-2517_36_10.362749/model-00012-0.81008-0.69231-4.44449-0.26000.h5\n",
      "23/23 [==============================] - 96s 4s/step - loss: 0.8101 - categorical_accuracy: 0.6923 - val_loss: 4.4445 - val_categorical_accuracy: 0.2600 - lr: 4.0000e-04\n",
      "Epoch 13/30\n",
      "22/23 [===========================>..] - ETA: 3s - loss: 0.7598 - categorical_accuracy: 0.7053\n",
      "Epoch 00013: saving model to model_init_2022-01-2517_36_10.362749/model-00013-0.76184-0.70513-4.20938-0.25000.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "23/23 [==============================] - 94s 4s/step - loss: 0.7618 - categorical_accuracy: 0.7051 - val_loss: 4.2094 - val_categorical_accuracy: 0.2500 - lr: 4.0000e-04\n",
      "Epoch 14/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.8438 - categorical_accuracy: 0.6855\n",
      "Epoch 00014: saving model to model_init_2022-01-2517_36_10.362749/model-00014-0.84375-0.68552-4.01434-0.25000.h5\n",
      "23/23 [==============================] - 94s 4s/step - loss: 0.8438 - categorical_accuracy: 0.6855 - val_loss: 4.0143 - val_categorical_accuracy: 0.2500 - lr: 8.0000e-05\n",
      "Epoch 15/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7670 - categorical_accuracy: 0.7157\n",
      "Epoch 00015: saving model to model_init_2022-01-2517_36_10.362749/model-00015-0.76702-0.71569-3.61123-0.27000.h5\n",
      "23/23 [==============================] - 103s 5s/step - loss: 0.7670 - categorical_accuracy: 0.7157 - val_loss: 3.6112 - val_categorical_accuracy: 0.2700 - lr: 8.0000e-05\n",
      "Epoch 16/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7483 - categorical_accuracy: 0.7270\n",
      "Epoch 00016: saving model to model_init_2022-01-2517_36_10.362749/model-00016-0.74830-0.72700-3.10880-0.29000.h5\n",
      "23/23 [==============================] - 104s 5s/step - loss: 0.7483 - categorical_accuracy: 0.7270 - val_loss: 3.1088 - val_categorical_accuracy: 0.2900 - lr: 8.0000e-05\n",
      "Epoch 17/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.8091 - categorical_accuracy: 0.7051\n",
      "Epoch 00017: saving model to model_init_2022-01-2517_36_10.362749/model-00017-0.80909-0.70513-2.87925-0.27000.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "23/23 [==============================] - 94s 4s/step - loss: 0.8091 - categorical_accuracy: 0.7051 - val_loss: 2.8793 - val_categorical_accuracy: 0.2700 - lr: 8.0000e-05\n",
      "Epoch 18/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7714 - categorical_accuracy: 0.6976\n",
      "Epoch 00018: saving model to model_init_2022-01-2517_36_10.362749/model-00018-0.77140-0.69759-2.62305-0.32000.h5\n",
      "23/23 [==============================] - 110s 5s/step - loss: 0.7714 - categorical_accuracy: 0.6976 - val_loss: 2.6230 - val_categorical_accuracy: 0.3200 - lr: 1.6000e-05\n",
      "Epoch 19/30\n",
      "22/23 [===========================>..] - ETA: 4s - loss: 0.7981 - categorical_accuracy: 0.7068\n",
      "Epoch 00019: saving model to model_init_2022-01-2517_36_10.362749/model-00019-0.79926-0.70588-2.21648-0.32000.h5\n",
      "23/23 [==============================] - 104s 5s/step - loss: 0.7993 - categorical_accuracy: 0.7059 - val_loss: 2.2165 - val_categorical_accuracy: 0.3200 - lr: 1.6000e-05\n",
      "Epoch 20/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.8236 - categorical_accuracy: 0.6893\n",
      "Epoch 00020: saving model to model_init_2022-01-2517_36_10.362749/model-00020-0.82365-0.68929-2.08295-0.38000.h5\n",
      "23/23 [==============================] - 101s 5s/step - loss: 0.8236 - categorical_accuracy: 0.6893 - val_loss: 2.0830 - val_categorical_accuracy: 0.3800 - lr: 1.6000e-05\n",
      "Epoch 21/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7567 - categorical_accuracy: 0.7225\n",
      "Epoch 00021: saving model to model_init_2022-01-2517_36_10.362749/model-00021-0.75666-0.72247-1.63410-0.49000.h5\n",
      "23/23 [==============================] - 94s 4s/step - loss: 0.7567 - categorical_accuracy: 0.7225 - val_loss: 1.6341 - val_categorical_accuracy: 0.4900 - lr: 1.6000e-05\n",
      "Epoch 22/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7699 - categorical_accuracy: 0.7157\n",
      "Epoch 00022: saving model to model_init_2022-01-2517_36_10.362749/model-00022-0.76993-0.71569-1.41980-0.60000.h5\n",
      "23/23 [==============================] - 102s 5s/step - loss: 0.7699 - categorical_accuracy: 0.7157 - val_loss: 1.4198 - val_categorical_accuracy: 0.6000 - lr: 1.6000e-05\n",
      "Epoch 23/30\n",
      "22/23 [===========================>..] - ETA: 4s - loss: 0.7162 - categorical_accuracy: 0.7318\n",
      "Epoch 00023: saving model to model_init_2022-01-2517_36_10.362749/model-00023-0.72191-0.72926-1.27265-0.56000.h5\n",
      "23/23 [==============================] - 103s 5s/step - loss: 0.7219 - categorical_accuracy: 0.7293 - val_loss: 1.2727 - val_categorical_accuracy: 0.5600 - lr: 1.6000e-05\n",
      "Epoch 24/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.8060 - categorical_accuracy: 0.6991\n",
      "Epoch 00024: saving model to model_init_2022-01-2517_36_10.362749/model-00024-0.80601-0.69910-1.03578-0.66000.h5\n",
      "23/23 [==============================] - 100s 5s/step - loss: 0.8060 - categorical_accuracy: 0.6991 - val_loss: 1.0358 - val_categorical_accuracy: 0.6600 - lr: 1.6000e-05\n",
      "Epoch 25/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.8134 - categorical_accuracy: 0.6953\n",
      "Epoch 00025: saving model to model_init_2022-01-2517_36_10.362749/model-00025-0.81341-0.69532-0.94049-0.67000.h5\n",
      "23/23 [==============================] - 95s 4s/step - loss: 0.8134 - categorical_accuracy: 0.6953 - val_loss: 0.9405 - val_categorical_accuracy: 0.6700 - lr: 1.6000e-05\n",
      "Epoch 26/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7747 - categorical_accuracy: 0.7149\n",
      "Epoch 00026: saving model to model_init_2022-01-2517_36_10.362749/model-00026-0.77470-0.71493-0.83983-0.69000.h5\n",
      "23/23 [==============================] - 98s 4s/step - loss: 0.7747 - categorical_accuracy: 0.7149 - val_loss: 0.8398 - val_categorical_accuracy: 0.6900 - lr: 1.6000e-05\n",
      "Epoch 27/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7760 - categorical_accuracy: 0.7089\n",
      "Epoch 00027: saving model to model_init_2022-01-2517_36_10.362749/model-00027-0.77596-0.70890-0.78443-0.69000.h5\n",
      "23/23 [==============================] - 100s 5s/step - loss: 0.7760 - categorical_accuracy: 0.7089 - val_loss: 0.7844 - val_categorical_accuracy: 0.6900 - lr: 1.6000e-05\n",
      "Epoch 28/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7076 - categorical_accuracy: 0.7376\n",
      "Epoch 00028: saving model to model_init_2022-01-2517_36_10.362749/model-00028-0.70757-0.73756-0.80684-0.70000.h5\n",
      "23/23 [==============================] - 99s 4s/step - loss: 0.7076 - categorical_accuracy: 0.7376 - val_loss: 0.8068 - val_categorical_accuracy: 0.7000 - lr: 1.6000e-05\n",
      "Epoch 29/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7759 - categorical_accuracy: 0.7149\n",
      "Epoch 00029: saving model to model_init_2022-01-2517_36_10.362749/model-00029-0.77586-0.71493-0.72727-0.72000.h5\n",
      "23/23 [==============================] - 95s 4s/step - loss: 0.7759 - categorical_accuracy: 0.7149 - val_loss: 0.7273 - val_categorical_accuracy: 0.7200 - lr: 1.6000e-05\n",
      "Epoch 30/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7592 - categorical_accuracy: 0.7006\n",
      "Epoch 00030: saving model to model_init_2022-01-2517_36_10.362749/model-00030-0.75916-0.70060-0.59067-0.74000.h5\n",
      "23/23 [==============================] - 100s 5s/step - loss: 0.7592 - categorical_accuracy: 0.7006 - val_loss: 0.5907 - val_categorical_accuracy: 0.7400 - lr: 1.6000e-05\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Params:\", conv_3d3_model.count_params())\n",
    "history_model3=conv_3d3.train_model(conv_3d3_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRCcRDCckMZ2"
   },
   "source": [
    "#### OBSERVATION:\n",
    "Overfitting solved but the accuracy is poor. Let’s try with more augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "En6L0QBn8RSh"
   },
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "XclEcbKJ8QJi"
   },
   "outputs": [],
   "source": [
    "class ModelBuilderMoreAugmentation(metaclass= abc.ABCMeta):\n",
    "    \n",
    "    def initialize_path(self,project_folder):\n",
    "        self.train_doc = np.random.permutation(open(project_folder + '/' + 'train.csv').readlines())\n",
    "        self.val_doc = np.random.permutation(open(project_folder + '/' + 'val.csv').readlines())\n",
    "        self.train_path = project_folder + '/' + 'train'\n",
    "        self.val_path =  project_folder + '/' + 'val'\n",
    "        self.num_train_sequences = len(self.train_doc)\n",
    "        self.num_val_sequences = len(self.val_doc)\n",
    "        \n",
    "    def initialize_image_properties(self,image_height=100,image_width=100):\n",
    "        self.image_height=image_height\n",
    "        self.image_width=image_width\n",
    "        self.channels=3\n",
    "        self.num_classes=5\n",
    "        self.total_frames=30\n",
    "          \n",
    "    def initialize_hyperparams(self,frames_to_sample=30,batch_size=20,num_epochs=20):\n",
    "        self.frames_to_sample=frames_to_sample\n",
    "        self.batch_size=batch_size\n",
    "        self.num_epochs=num_epochs\n",
    "        \n",
    "        \n",
    "    def generator(self,source_path, folder_list, augment=False):\n",
    "        img_idx = np.round(np.linspace(0,self.total_frames-1,self.frames_to_sample)).astype(int)\n",
    "        batch_size=self.batch_size\n",
    "        while True:\n",
    "            t = np.random.permutation(folder_list)\n",
    "            num_batches = len(t)//batch_size\n",
    "        \n",
    "            for batch in range(num_batches): \n",
    "                batch_data, batch_labels= self.one_batch_data(source_path,t,batch,batch_size,img_idx,augment)\n",
    "                yield batch_data, batch_labels \n",
    "\n",
    "            remaining_seq=len(t)%batch_size\n",
    "        \n",
    "            if (remaining_seq != 0):\n",
    "                batch_data, batch_labels= self.one_batch_data(source_path,t,num_batches,batch_size,img_idx,augment,remaining_seq)\n",
    "                yield batch_data, batch_labels \n",
    "    \n",
    "    \n",
    "    def one_batch_data(self,source_path,t,batch,batch_size,img_idx,augment,remaining_seq=0):\n",
    "    \n",
    "        seq_len = remaining_seq if remaining_seq else batch_size\n",
    "    \n",
    "        batch_data = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels)) \n",
    "        batch_labels = np.zeros((seq_len,self.num_classes)) \n",
    "    \n",
    "        if (augment): batch_data_aug = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels))\n",
    "\n",
    "        \n",
    "        for folder in range(seq_len): \n",
    "            imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) \n",
    "            for idx,item in enumerate(img_idx): \n",
    "                image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                image_resized=imresize(image,(self.image_height,self.image_width,3))\n",
    "            \n",
    "\n",
    "                batch_data[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n",
    "                batch_data[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n",
    "                batch_data[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n",
    "            \n",
    "                if (augment):\n",
    "                    shifted = cv2.warpAffine(image, \n",
    "                                             np.float32([[1, 0, np.random.randint(-30,30)],[0, 1, np.random.randint(-30,30)]]), \n",
    "                                            (image.shape[1], image.shape[0]))\n",
    "                    \n",
    "                    gray = cv2.cvtColor(shifted,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                    x0, y0 = np.argwhere(gray > 0).min(axis=0)\n",
    "                    x1, y1 = np.argwhere(gray > 0).max(axis=0) \n",
    "                    \n",
    "                    cropped=shifted[x0:x1,y0:y1,:]\n",
    "                    \n",
    "                    image_resized=imresize(cropped,(self.image_height,self.image_width,3))\n",
    "                    \n",
    "                    M = cv2.getRotationMatrix2D((self.image_width//2,self.image_height//2),\n",
    "                                                np.random.randint(-10,10), 1.0)\n",
    "                    rotated = cv2.warpAffine(image_resized, M, (self.image_width, self.image_height))\n",
    "                    \n",
    "                    #shifted = cv2.warpAffine(image_resized, \n",
    "                    #                        np.float32([[1, 0, np.random.randint(-3,3)],[0, 1, np.random.randint(-3,3)]]), \n",
    "                    #                        (image_resized.shape[1], image_resized.shape[0]))\n",
    "            \n",
    "                    batch_data_aug[folder,idx,:,:,0] = (rotated[:,:,0])/255\n",
    "                    batch_data_aug[folder,idx,:,:,1] = (rotated[:,:,1])/255\n",
    "                    batch_data_aug[folder,idx,:,:,2] = (rotated[:,:,2])/255\n",
    "                \n",
    "            \n",
    "            batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            \n",
    "    \n",
    "        if (augment):\n",
    "            batch_data=np.concatenate([batch_data,batch_data_aug])\n",
    "            batch_labels=np.concatenate([batch_labels,batch_labels])\n",
    "\n",
    "        \n",
    "        return(batch_data,batch_labels)\n",
    "    \n",
    "    \n",
    "    def train_model(self, model, augment_data=False):\n",
    "        train_generator = self.generator(self.train_path, self.train_doc,augment=augment_data)\n",
    "        val_generator = self.generator(self.val_path, self.val_doc)\n",
    "\n",
    "        model_name = 'model_init' + '_' + str(datetime.datetime.now()).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "        if not os.path.exists(model_name):\n",
    "            os.mkdir(model_name)\n",
    "        \n",
    "        # filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "        filepath = F\"/content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/\"+ model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "        LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)\n",
    "        callbacks_list = [checkpoint, LR]\n",
    "\n",
    "        if (self.num_train_sequences%self.batch_size) == 0:\n",
    "            steps_per_epoch = int(self.num_train_sequences/self.batch_size)\n",
    "        else:\n",
    "            steps_per_epoch = (self.num_train_sequences//self.batch_size) + 1\n",
    "\n",
    "        if (self.num_val_sequences%self.batch_size) == 0:\n",
    "            validation_steps = int(self.num_val_sequences/self.batch_size)\n",
    "        else:\n",
    "            validation_steps = (self.num_val_sequences//self.batch_size) + 1\n",
    "    \n",
    "        history=model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=self.num_epochs, verbose=1, \n",
    "                            callbacks=callbacks_list, validation_data=val_generator, \n",
    "                            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
    "        return history\n",
    "\n",
    "        \n",
    "    @abc.abstractmethod\n",
    "    def define_model(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UF-1sudafNjP"
   },
   "source": [
    "## Model 3\n",
    "\n",
    "With more augmentation and increasing filter & resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "9PKr3-oifQUE"
   },
   "outputs": [],
   "source": [
    "class ModelConv3D10(ModelBuilderMoreAugmentation):\n",
    "    \n",
    "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv3D(16, filtersize, padding='same',\n",
    "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(32, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(64, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(128, filtersize, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "        model.add(Dense(self.num_classes,activation='softmax'))\n",
    "\n",
    "        optimiser = 'adam'\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ph0aW3wIfneC",
    "outputId": "c67b4e5b-8739-408b-9841-6e9715344cb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_4 (Conv3D)           (None, 20, 160, 160, 16)  1312      \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 20, 160, 160, 16)  0         \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 20, 160, 160, 16)  64       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_4 (MaxPooling  (None, 10, 80, 80, 16)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_5 (Conv3D)           (None, 10, 80, 80, 32)    13856     \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 10, 80, 80, 32)    0         \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 10, 80, 80, 32)   128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_5 (MaxPooling  (None, 5, 40, 40, 32)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_6 (Conv3D)           (None, 5, 40, 40, 64)     55360     \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 5, 40, 40, 64)     0         \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 5, 40, 40, 64)    256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_6 (MaxPooling  (None, 2, 20, 20, 64)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_7 (Conv3D)           (None, 2, 20, 20, 128)    221312    \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 2, 20, 20, 128)    0         \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 2, 20, 20, 128)   512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_7 (MaxPooling  (None, 1, 10, 10, 128)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 12800)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               3277056   \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,638,981\n",
      "Trainable params: 3,637,477\n",
      "Non-trainable params: 1,504\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_3d10=ModelConv3D10()\n",
    "conv_3d10.initialize_path(project_folder)\n",
    "conv_3d10.initialize_image_properties(image_height=160,image_width=160)\n",
    "conv_3d10.initialize_hyperparams(frames_to_sample=20,batch_size=20,num_epochs=30)\n",
    "conv_3d10_model=conv_3d10.define_model(dense_neurons=256,dropout=0.5)\n",
    "conv_3d10_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EOO6jOvwfqoi",
    "outputId": "c188e356-6737-489f-eb62-c31388c99588"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 3638981\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:130: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: DeprecationWarning:     `imread` is deprecated!\n",
      "    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``imageio.imread`` instead.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:56: DeprecationWarning:     `imresize` is deprecated!\n",
      "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``skimage.transform.resize`` instead.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:75: DeprecationWarning:     `imresize` is deprecated!\n",
      "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.9769 - categorical_accuracy: 0.3846 \n",
      "Epoch 00001: val_loss improved from inf to 4.66478, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2610_53_41.462096/model-00001-1.97687-0.38462-4.66478-0.16000.h5\n",
      "34/34 [==============================] - 1853s 56s/step - loss: 1.9769 - categorical_accuracy: 0.3846 - val_loss: 4.6648 - val_categorical_accuracy: 0.1600 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.5012 - categorical_accuracy: 0.4774\n",
      "Epoch 00002: val_loss did not improve from 4.66478\n",
      "34/34 [==============================] - 145s 4s/step - loss: 1.5012 - categorical_accuracy: 0.4774 - val_loss: 11.8687 - val_categorical_accuracy: 0.1500 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.3918 - categorical_accuracy: 0.5196\n",
      "Epoch 00003: val_loss did not improve from 4.66478\n",
      "34/34 [==============================] - 143s 4s/step - loss: 1.3918 - categorical_accuracy: 0.5196 - val_loss: 11.5069 - val_categorical_accuracy: 0.1900 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.1910 - categorical_accuracy: 0.5664\n",
      "Epoch 00004: val_loss did not improve from 4.66478\n",
      "34/34 [==============================] - 142s 4s/step - loss: 1.1910 - categorical_accuracy: 0.5664 - val_loss: 9.8404 - val_categorical_accuracy: 0.2000 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.2108 - categorical_accuracy: 0.5694\n",
      "Epoch 00005: val_loss improved from 4.66478 to 4.23941, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2610_53_41.462096/model-00005-1.21085-0.56938-4.23941-0.21000.h5\n",
      "34/34 [==============================] - 143s 4s/step - loss: 1.2108 - categorical_accuracy: 0.5694 - val_loss: 4.2394 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.1763 - categorical_accuracy: 0.5905\n",
      "Epoch 00006: val_loss did not improve from 4.23941\n",
      "34/34 [==============================] - 139s 4s/step - loss: 1.1763 - categorical_accuracy: 0.5905 - val_loss: 6.1407 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.4590 - categorical_accuracy: 0.4781\n",
      "Epoch 00007: val_loss improved from 4.23941 to 2.61335, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2610_53_41.462096/model-00007-1.45902-0.47813-2.61335-0.36000.h5\n",
      "34/34 [==============================] - 136s 4s/step - loss: 1.4590 - categorical_accuracy: 0.4781 - val_loss: 2.6133 - val_categorical_accuracy: 0.3600 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.1365 - categorical_accuracy: 0.5762\n",
      "Epoch 00008: val_loss improved from 2.61335 to 2.01190, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2610_53_41.462096/model-00008-1.13649-0.57617-2.01190-0.40000.h5\n",
      "34/34 [==============================] - 136s 4s/step - loss: 1.1365 - categorical_accuracy: 0.5762 - val_loss: 2.0119 - val_categorical_accuracy: 0.4000 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.0450 - categorical_accuracy: 0.5980\n",
      "Epoch 00009: val_loss did not improve from 2.01190\n",
      "34/34 [==============================] - 142s 4s/step - loss: 1.0450 - categorical_accuracy: 0.5980 - val_loss: 2.7190 - val_categorical_accuracy: 0.2800 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.9926 - categorical_accuracy: 0.6244\n",
      "Epoch 00010: val_loss improved from 2.01190 to 1.50679, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2610_53_41.462096/model-00010-0.99263-0.62443-1.50679-0.46000.h5\n",
      "34/34 [==============================] - 144s 4s/step - loss: 0.9926 - categorical_accuracy: 0.6244 - val_loss: 1.5068 - val_categorical_accuracy: 0.4600 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.9251 - categorical_accuracy: 0.6365\n",
      "Epoch 00011: val_loss improved from 1.50679 to 1.17817, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2610_53_41.462096/model-00011-0.92507-0.63650-1.17817-0.53000.h5\n",
      "34/34 [==============================] - 139s 4s/step - loss: 0.9251 - categorical_accuracy: 0.6365 - val_loss: 1.1782 - val_categorical_accuracy: 0.5300 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.7456 - categorical_accuracy: 0.7074\n",
      "Epoch 00012: val_loss did not improve from 1.17817\n",
      "34/34 [==============================] - 143s 4s/step - loss: 0.7456 - categorical_accuracy: 0.7074 - val_loss: 1.5520 - val_categorical_accuracy: 0.5300 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.7597 - categorical_accuracy: 0.7202\n",
      "Epoch 00013: val_loss did not improve from 1.17817\n",
      "34/34 [==============================] - 150s 5s/step - loss: 0.7597 - categorical_accuracy: 0.7202 - val_loss: 1.6735 - val_categorical_accuracy: 0.4400 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.6421 - categorical_accuracy: 0.7443\n",
      "Epoch 00014: val_loss improved from 1.17817 to 1.07098, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2610_53_41.462096/model-00014-0.64211-0.74434-1.07098-0.57000.h5\n",
      "34/34 [==============================] - 143s 4s/step - loss: 0.6421 - categorical_accuracy: 0.7443 - val_loss: 1.0710 - val_categorical_accuracy: 0.5700 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.6455 - categorical_accuracy: 0.7526\n",
      "Epoch 00015: val_loss did not improve from 1.07098\n",
      "34/34 [==============================] - 143s 4s/step - loss: 0.6455 - categorical_accuracy: 0.7526 - val_loss: 1.1131 - val_categorical_accuracy: 0.5900 - lr: 0.0010\n",
      "Epoch 16/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.6407 - categorical_accuracy: 0.7496\n",
      "Epoch 00016: val_loss improved from 1.07098 to 0.85294, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2610_53_41.462096/model-00016-0.64071-0.74962-0.85294-0.69000.h5\n",
      "34/34 [==============================] - 143s 4s/step - loss: 0.6407 - categorical_accuracy: 0.7496 - val_loss: 0.8529 - val_categorical_accuracy: 0.6900 - lr: 0.0010\n",
      "Epoch 17/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.7981 - categorical_accuracy: 0.6916\n",
      "Epoch 00017: val_loss did not improve from 0.85294\n",
      "34/34 [==============================] - 148s 4s/step - loss: 0.7981 - categorical_accuracy: 0.6916 - val_loss: 1.0601 - val_categorical_accuracy: 0.5600 - lr: 0.0010\n",
      "Epoch 18/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.6535 - categorical_accuracy: 0.7504\n",
      "Epoch 00018: val_loss did not improve from 0.85294\n",
      "34/34 [==============================] - 139s 4s/step - loss: 0.6535 - categorical_accuracy: 0.7504 - val_loss: 1.7923 - val_categorical_accuracy: 0.4500 - lr: 0.0010\n",
      "Epoch 19/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5565 - categorical_accuracy: 0.7896\n",
      "Epoch 00019: val_loss did not improve from 0.85294\n",
      "34/34 [==============================] - 144s 4s/step - loss: 0.5565 - categorical_accuracy: 0.7896 - val_loss: 0.8936 - val_categorical_accuracy: 0.6200 - lr: 0.0010\n",
      "Epoch 20/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5286 - categorical_accuracy: 0.8002\n",
      "Epoch 00020: val_loss did not improve from 0.85294\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "34/34 [==============================] - 144s 4s/step - loss: 0.5286 - categorical_accuracy: 0.8002 - val_loss: 1.3249 - val_categorical_accuracy: 0.5900 - lr: 0.0010\n",
      "Epoch 21/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4571 - categorical_accuracy: 0.8250\n",
      "Epoch 00021: val_loss improved from 0.85294 to 0.78010, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2610_53_41.462096/model-00021-0.45706-0.82504-0.78010-0.71000.h5\n",
      "34/34 [==============================] - 138s 4s/step - loss: 0.4571 - categorical_accuracy: 0.8250 - val_loss: 0.7801 - val_categorical_accuracy: 0.7100 - lr: 2.0000e-04\n",
      "Epoch 22/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3926 - categorical_accuracy: 0.8567\n",
      "Epoch 00022: val_loss improved from 0.78010 to 0.77771, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2610_53_41.462096/model-00022-0.39262-0.85671-0.77771-0.70000.h5\n",
      "34/34 [==============================] - 146s 4s/step - loss: 0.3926 - categorical_accuracy: 0.8567 - val_loss: 0.7777 - val_categorical_accuracy: 0.7000 - lr: 2.0000e-04\n",
      "Epoch 23/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4101 - categorical_accuracy: 0.8326\n",
      "Epoch 00023: val_loss improved from 0.77771 to 0.54017, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2610_53_41.462096/model-00023-0.41013-0.83258-0.54017-0.76000.h5\n",
      "34/34 [==============================] - 136s 4s/step - loss: 0.4101 - categorical_accuracy: 0.8326 - val_loss: 0.5402 - val_categorical_accuracy: 0.7600 - lr: 2.0000e-04\n",
      "Epoch 24/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3869 - categorical_accuracy: 0.8612\n",
      "Epoch 00024: val_loss did not improve from 0.54017\n",
      "34/34 [==============================] - 144s 4s/step - loss: 0.3869 - categorical_accuracy: 0.8612 - val_loss: 0.6072 - val_categorical_accuracy: 0.8300 - lr: 2.0000e-04\n",
      "Epoch 25/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4115 - categorical_accuracy: 0.8567\n",
      "Epoch 00025: val_loss did not improve from 0.54017\n",
      "34/34 [==============================] - 138s 4s/step - loss: 0.4115 - categorical_accuracy: 0.8567 - val_loss: 0.5758 - val_categorical_accuracy: 0.7600 - lr: 2.0000e-04\n",
      "Epoch 26/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3580 - categorical_accuracy: 0.8710\n",
      "Epoch 00026: val_loss did not improve from 0.54017\n",
      "34/34 [==============================] - 138s 4s/step - loss: 0.3580 - categorical_accuracy: 0.8710 - val_loss: 0.7015 - val_categorical_accuracy: 0.7100 - lr: 2.0000e-04\n",
      "Epoch 27/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4197 - categorical_accuracy: 0.8424\n",
      "Epoch 00027: val_loss improved from 0.54017 to 0.46834, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2610_53_41.462096/model-00027-0.41966-0.84238-0.46834-0.79000.h5\n",
      "34/34 [==============================] - 142s 4s/step - loss: 0.4197 - categorical_accuracy: 0.8424 - val_loss: 0.4683 - val_categorical_accuracy: 0.7900 - lr: 2.0000e-04\n",
      "Epoch 28/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3272 - categorical_accuracy: 0.8763\n",
      "Epoch 00028: val_loss did not improve from 0.46834\n",
      "34/34 [==============================] - 145s 4s/step - loss: 0.3272 - categorical_accuracy: 0.8763 - val_loss: 0.5418 - val_categorical_accuracy: 0.7800 - lr: 2.0000e-04\n",
      "Epoch 29/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3258 - categorical_accuracy: 0.8906\n",
      "Epoch 00029: val_loss did not improve from 0.46834\n",
      "34/34 [==============================] - 138s 4s/step - loss: 0.3258 - categorical_accuracy: 0.8906 - val_loss: 0.5295 - val_categorical_accuracy: 0.8200 - lr: 2.0000e-04\n",
      "Epoch 30/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3325 - categorical_accuracy: 0.8771\n",
      "Epoch 00030: val_loss did not improve from 0.46834\n",
      "34/34 [==============================] - 143s 4s/step - loss: 0.3325 - categorical_accuracy: 0.8771 - val_loss: 0.6497 - val_categorical_accuracy: 0.7600 - lr: 2.0000e-04\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Params:\", conv_3d10_model.count_params())\n",
    "history_model10=conv_3d10.train_model(conv_3d10_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lCr8TzHvWWx"
   },
   "source": [
    "#### OBSERVATION\n",
    "Accuracy improved, trying with more augmentation & reduced network parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NrI6ZN0Gf5fy"
   },
   "source": [
    "## Model 4\n",
    "Trying with reduced parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "JdCyodUPf7yE"
   },
   "outputs": [],
   "source": [
    "class ModelConv3D16(ModelBuilderMoreAugmentation):\n",
    "    \n",
    "    def define_model(self,dense_neurons=64,dropout=0.25):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv3D(8, (3, 3, 3), padding='same',\n",
    "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(16, (3, 3, 3), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Dense(self.num_classes,activation='softmax'))\n",
    "\n",
    "        optimiser = 'adam'\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ea5uunlxgARn",
    "outputId": "7087109f-b794-4d65-9342-801cb4c82e46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d (Conv3D)             (None, 16, 120, 120, 8)   656       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 16, 120, 120, 8)   0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 16, 120, 120, 8)  32        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling3d (MaxPooling3D  (None, 8, 60, 60, 8)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv3d_1 (Conv3D)           (None, 8, 60, 60, 16)     3472      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 8, 60, 60, 16)     0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 8, 60, 60, 16)    64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_1 (MaxPooling  (None, 4, 30, 30, 16)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_2 (Conv3D)           (None, 4, 30, 30, 32)     4128      \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 4, 30, 30, 32)     0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 4, 30, 30, 32)    128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_2 (MaxPooling  (None, 2, 15, 15, 32)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_3 (Conv3D)           (None, 2, 15, 15, 64)     16448     \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 2, 15, 15, 64)     0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 2, 15, 15, 64)    256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_3 (MaxPooling  (None, 1, 7, 7, 64)      0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                200768    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 230,949\n",
      "Trainable params: 230,453\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_3d16=ModelConv3D16()\n",
    "conv_3d16.initialize_path(project_folder)\n",
    "conv_3d16.initialize_image_properties(image_height=120,image_width=120)\n",
    "conv_3d16.initialize_hyperparams(frames_to_sample=16,batch_size=20,num_epochs=30)\n",
    "conv_3d16_model=conv_3d16.define_model(dense_neurons=64,dropout=0.25)\n",
    "conv_3d16_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nx17f8w7gDXL",
    "outputId": "1f7633ca-bb49-4590-a9fb-0eb922a15161"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 230949\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:130: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: DeprecationWarning:     `imread` is deprecated!\n",
      "    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``imageio.imread`` instead.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:56: DeprecationWarning:     `imresize` is deprecated!\n",
      "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``skimage.transform.resize`` instead.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:75: DeprecationWarning:     `imresize` is deprecated!\n",
      "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.6579 - categorical_accuracy: 0.3839  \n",
      "Epoch 00001: val_loss improved from inf to 1.99462, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2613_32_21.885473/model-00001-1.65789-0.38386-1.99462-0.16000.h5\n",
      "34/34 [==============================] - 3070s 93s/step - loss: 1.6579 - categorical_accuracy: 0.3839 - val_loss: 1.9946 - val_categorical_accuracy: 0.1600 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.2327 - categorical_accuracy: 0.5075\n",
      "Epoch 00002: val_loss did not improve from 1.99462\n",
      "34/34 [==============================] - 125s 4s/step - loss: 1.2327 - categorical_accuracy: 0.5075 - val_loss: 3.8271 - val_categorical_accuracy: 0.1300 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.9625 - categorical_accuracy: 0.6222\n",
      "Epoch 00003: val_loss did not improve from 1.99462\n",
      "34/34 [==============================] - 129s 4s/step - loss: 0.9625 - categorical_accuracy: 0.6222 - val_loss: 5.9153 - val_categorical_accuracy: 0.1300 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.8535 - categorical_accuracy: 0.6780\n",
      "Epoch 00004: val_loss did not improve from 1.99462\n",
      "34/34 [==============================] - 126s 4s/step - loss: 0.8535 - categorical_accuracy: 0.6780 - val_loss: 6.2449 - val_categorical_accuracy: 0.1800 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.7930 - categorical_accuracy: 0.6870\n",
      "Epoch 00005: val_loss did not improve from 1.99462\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "34/34 [==============================] - 124s 4s/step - loss: 0.7930 - categorical_accuracy: 0.6870 - val_loss: 5.6797 - val_categorical_accuracy: 0.1700 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.7024 - categorical_accuracy: 0.7368\n",
      "Epoch 00006: val_loss did not improve from 1.99462\n",
      "34/34 [==============================] - 125s 4s/step - loss: 0.7024 - categorical_accuracy: 0.7368 - val_loss: 5.5122 - val_categorical_accuracy: 0.1600 - lr: 2.0000e-04\n",
      "Epoch 7/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.6398 - categorical_accuracy: 0.7609\n",
      "Epoch 00007: val_loss did not improve from 1.99462\n",
      "34/34 [==============================] - 125s 4s/step - loss: 0.6398 - categorical_accuracy: 0.7609 - val_loss: 5.5989 - val_categorical_accuracy: 0.1800 - lr: 2.0000e-04\n",
      "Epoch 8/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.6087 - categorical_accuracy: 0.7783\n",
      "Epoch 00008: val_loss did not improve from 1.99462\n",
      "34/34 [==============================] - 125s 4s/step - loss: 0.6087 - categorical_accuracy: 0.7783 - val_loss: 5.4013 - val_categorical_accuracy: 0.1300 - lr: 2.0000e-04\n",
      "Epoch 9/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5693 - categorical_accuracy: 0.7934\n",
      "Epoch 00009: val_loss did not improve from 1.99462\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "34/34 [==============================] - 128s 4s/step - loss: 0.5693 - categorical_accuracy: 0.7934 - val_loss: 4.5010 - val_categorical_accuracy: 0.1700 - lr: 2.0000e-04\n",
      "Epoch 10/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5658 - categorical_accuracy: 0.8024\n",
      "Epoch 00010: val_loss did not improve from 1.99462\n",
      "34/34 [==============================] - 126s 4s/step - loss: 0.5658 - categorical_accuracy: 0.8024 - val_loss: 4.1008 - val_categorical_accuracy: 0.2000 - lr: 4.0000e-05\n",
      "Epoch 11/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5989 - categorical_accuracy: 0.7903\n",
      "Epoch 00011: val_loss did not improve from 1.99462\n",
      "34/34 [==============================] - 125s 4s/step - loss: 0.5989 - categorical_accuracy: 0.7903 - val_loss: 3.5340 - val_categorical_accuracy: 0.1900 - lr: 4.0000e-05\n",
      "Epoch 12/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5471 - categorical_accuracy: 0.7851\n",
      "Epoch 00012: val_loss did not improve from 1.99462\n",
      "34/34 [==============================] - 126s 4s/step - loss: 0.5471 - categorical_accuracy: 0.7851 - val_loss: 2.8274 - val_categorical_accuracy: 0.2300 - lr: 4.0000e-05\n",
      "Epoch 13/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5402 - categorical_accuracy: 0.8039\n",
      "Epoch 00013: val_loss did not improve from 1.99462\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "34/34 [==============================] - 127s 4s/step - loss: 0.5402 - categorical_accuracy: 0.8039 - val_loss: 2.2980 - val_categorical_accuracy: 0.3400 - lr: 4.0000e-05\n",
      "Epoch 14/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5469 - categorical_accuracy: 0.7971\n",
      "Epoch 00014: val_loss improved from 1.99462 to 1.81588, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2613_32_21.885473/model-00014-0.54693-0.79713-1.81588-0.49000.h5\n",
      "34/34 [==============================] - 125s 4s/step - loss: 0.5469 - categorical_accuracy: 0.7971 - val_loss: 1.8159 - val_categorical_accuracy: 0.4900 - lr: 8.0000e-06\n",
      "Epoch 15/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5486 - categorical_accuracy: 0.8039\n",
      "Epoch 00015: val_loss improved from 1.81588 to 1.58771, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2613_32_21.885473/model-00015-0.54864-0.80392-1.58771-0.50000.h5\n",
      "34/34 [==============================] - 125s 4s/step - loss: 0.5486 - categorical_accuracy: 0.8039 - val_loss: 1.5877 - val_categorical_accuracy: 0.5000 - lr: 8.0000e-06\n",
      "Epoch 16/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5221 - categorical_accuracy: 0.8137\n",
      "Epoch 00016: val_loss improved from 1.58771 to 1.34856, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2613_32_21.885473/model-00016-0.52207-0.81373-1.34856-0.56000.h5\n",
      "34/34 [==============================] - 128s 4s/step - loss: 0.5221 - categorical_accuracy: 0.8137 - val_loss: 1.3486 - val_categorical_accuracy: 0.5600 - lr: 8.0000e-06\n",
      "Epoch 17/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5116 - categorical_accuracy: 0.8145\n",
      "Epoch 00017: val_loss improved from 1.34856 to 0.99633, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2613_32_21.885473/model-00017-0.51157-0.81448-0.99633-0.69000.h5\n",
      "34/34 [==============================] - 129s 4s/step - loss: 0.5116 - categorical_accuracy: 0.8145 - val_loss: 0.9963 - val_categorical_accuracy: 0.6900 - lr: 8.0000e-06\n",
      "Epoch 18/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5431 - categorical_accuracy: 0.8039\n",
      "Epoch 00018: val_loss did not improve from 0.99633\n",
      "34/34 [==============================] - 128s 4s/step - loss: 0.5431 - categorical_accuracy: 0.8039 - val_loss: 1.0709 - val_categorical_accuracy: 0.6600 - lr: 8.0000e-06\n",
      "Epoch 19/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5627 - categorical_accuracy: 0.7866\n",
      "Epoch 00019: val_loss improved from 0.99633 to 0.70934, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2613_32_21.885473/model-00019-0.56269-0.78658-0.70934-0.75000.h5\n",
      "34/34 [==============================] - 125s 4s/step - loss: 0.5627 - categorical_accuracy: 0.7866 - val_loss: 0.7093 - val_categorical_accuracy: 0.7500 - lr: 8.0000e-06\n",
      "Epoch 20/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5317 - categorical_accuracy: 0.8077\n",
      "Epoch 00020: val_loss did not improve from 0.70934\n",
      "34/34 [==============================] - 125s 4s/step - loss: 0.5317 - categorical_accuracy: 0.8077 - val_loss: 0.8330 - val_categorical_accuracy: 0.6900 - lr: 8.0000e-06\n",
      "Epoch 21/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5410 - categorical_accuracy: 0.8092\n",
      "Epoch 00021: val_loss did not improve from 0.70934\n",
      "34/34 [==============================] - 127s 4s/step - loss: 0.5410 - categorical_accuracy: 0.8092 - val_loss: 0.8364 - val_categorical_accuracy: 0.7300 - lr: 8.0000e-06\n",
      "Epoch 22/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4967 - categorical_accuracy: 0.8152\n",
      "Epoch 00022: val_loss did not improve from 0.70934\n",
      "34/34 [==============================] - 125s 4s/step - loss: 0.4967 - categorical_accuracy: 0.8152 - val_loss: 0.8620 - val_categorical_accuracy: 0.7200 - lr: 8.0000e-06\n",
      "Epoch 23/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4965 - categorical_accuracy: 0.8416\n",
      "Epoch 00023: val_loss did not improve from 0.70934\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "34/34 [==============================] - 125s 4s/step - loss: 0.4965 - categorical_accuracy: 0.8416 - val_loss: 0.8360 - val_categorical_accuracy: 0.7100 - lr: 8.0000e-06\n",
      "Epoch 24/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5344 - categorical_accuracy: 0.8039\n",
      "Epoch 00024: val_loss did not improve from 0.70934\n",
      "34/34 [==============================] - 125s 4s/step - loss: 0.5344 - categorical_accuracy: 0.8039 - val_loss: 0.7796 - val_categorical_accuracy: 0.7300 - lr: 1.6000e-06\n",
      "Epoch 25/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4908 - categorical_accuracy: 0.8311\n",
      "Epoch 00025: val_loss did not improve from 0.70934\n",
      "34/34 [==============================] - 123s 4s/step - loss: 0.4908 - categorical_accuracy: 0.8311 - val_loss: 0.7731 - val_categorical_accuracy: 0.7400 - lr: 1.6000e-06\n",
      "Epoch 26/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5317 - categorical_accuracy: 0.8175\n",
      "Epoch 00026: val_loss did not improve from 0.70934\n",
      "34/34 [==============================] - 124s 4s/step - loss: 0.5317 - categorical_accuracy: 0.8175 - val_loss: 0.7559 - val_categorical_accuracy: 0.7300 - lr: 1.6000e-06\n",
      "Epoch 27/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4956 - categorical_accuracy: 0.8220\n",
      "Epoch 00027: val_loss did not improve from 0.70934\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "34/34 [==============================] - 123s 4s/step - loss: 0.4956 - categorical_accuracy: 0.8220 - val_loss: 0.7103 - val_categorical_accuracy: 0.7500 - lr: 1.6000e-06\n",
      "Epoch 28/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5096 - categorical_accuracy: 0.8228\n",
      "Epoch 00028: val_loss did not improve from 0.70934\n",
      "34/34 [==============================] - 125s 4s/step - loss: 0.5096 - categorical_accuracy: 0.8228 - val_loss: 0.7534 - val_categorical_accuracy: 0.7300 - lr: 3.2000e-07\n",
      "Epoch 29/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5253 - categorical_accuracy: 0.7994\n",
      "Epoch 00029: val_loss did not improve from 0.70934\n",
      "34/34 [==============================] - 126s 4s/step - loss: 0.5253 - categorical_accuracy: 0.7994 - val_loss: 0.7406 - val_categorical_accuracy: 0.7200 - lr: 3.2000e-07\n",
      "Epoch 30/30\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5194 - categorical_accuracy: 0.8175\n",
      "Epoch 00030: val_loss improved from 0.70934 to 0.68613, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2613_32_21.885473/model-00030-0.51941-0.81750-0.68613-0.75000.h5\n",
      "34/34 [==============================] - 125s 4s/step - loss: 0.5194 - categorical_accuracy: 0.8175 - val_loss: 0.6861 - val_categorical_accuracy: 0.7500 - lr: 3.2000e-07\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Params:\", conv_3d16_model.count_params())\n",
    "history_model16=conv_3d16.train_model(conv_3d16_model,augment_data=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pt8YajjzAzzU"
   },
   "source": [
    "#### OBSERVATION\n",
    "Stable performance but the accuracy has decreased so let’s try with CNN + RNN type models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYrDaSgXUfxN"
   },
   "source": [
    "Model 5 - RNN-CNN-1 (LSTM Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imx75q8sUajE"
   },
   "outputs": [],
   "source": [
    "class RNNCNN1(ModelBuilder):\n",
    "    \n",
    "    def define_model(self,lstm_cells=64,dense_neurons=64,dropout=0.25):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),\n",
    "                                  input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "        model.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "        model.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "        model.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "        model.add(TimeDistributed(Conv2D(256, (3, 3) , padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "        #model.add(TimeDistributed(Conv2D(512, (2, 2) , padding='valid', activation='relu')))\n",
    "       # model.add(TimeDistributed(BatchNormalization()))\n",
    "       # model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "        model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "        model.add(LSTM(lstm_cells))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "        # optimiser = 'sgd'\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x5fNrWHZVwaS",
    "outputId": "5f1c321d-1c3f-4a81-ac29-ec17bdc67074"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_16 (TimeDi  (None, 18, 120, 120, 16)  448      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_17 (TimeDi  (None, 18, 120, 120, 16)  64       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_18 (TimeDi  (None, 18, 60, 60, 16)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_19 (TimeDi  (None, 18, 60, 60, 32)   4640      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_20 (TimeDi  (None, 18, 60, 60, 32)   128       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_21 (TimeDi  (None, 18, 30, 30, 32)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_22 (TimeDi  (None, 18, 30, 30, 64)   18496     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_23 (TimeDi  (None, 18, 30, 30, 64)   256       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_24 (TimeDi  (None, 18, 15, 15, 64)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_25 (TimeDi  (None, 18, 15, 15, 128)  73856     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_26 (TimeDi  (None, 18, 15, 15, 128)  512       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_27 (TimeDi  (None, 18, 7, 7, 128)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_28 (TimeDi  (None, 18, 7, 7, 256)    295168    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_29 (TimeDi  (None, 18, 7, 7, 256)    1024      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_30 (TimeDi  (None, 18, 3, 3, 256)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_31 (TimeDi  (None, 18, 2304)         0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               1245696   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,657,445\n",
      "Trainable params: 1,656,453\n",
      "Non-trainable params: 992\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_cnn1=RNNCNN1()\n",
    "rnn_cnn1.initialize_path(project_folder)\n",
    "rnn_cnn1.initialize_image_properties(image_height=120,image_width=120)\n",
    "rnn_cnn1.initialize_hyperparams(frames_to_sample=18,batch_size=20,num_epochs=20)\n",
    "rnn_cnn1_model=rnn_cnn1.define_model(lstm_cells=128,dense_neurons=128,dropout=0.25)\n",
    "rnn_cnn1_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yp8NzSDiWqAt",
    "outputId": "45ea239f-130b-49c6-d26c-268b5ca18faa"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 1657445\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:125: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: DeprecationWarning:     `imread` is deprecated!\n",
      "    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``imageio.imread`` instead.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:56: DeprecationWarning:     `imresize` is deprecated!\n",
      "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``skimage.transform.resize`` instead.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:75: DeprecationWarning:     `imresize` is deprecated!\n",
      "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.4779 - categorical_accuracy: 0.3658  \n",
      "Epoch 00001: saving model to model_init_2022-01-2503_50_29.368234/model-00001-1.47790-0.36576-1.79946-0.16000.h5\n",
      "34/34 [==============================] - 2990s 90s/step - loss: 1.4779 - categorical_accuracy: 0.3658 - val_loss: 1.7995 - val_categorical_accuracy: 0.1600 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.1439 - categorical_accuracy: 0.5452 \n",
      "Epoch 00002: saving model to model_init_2022-01-2503_50_29.368234/model-00002-1.14391-0.54525-2.22882-0.14000.h5\n",
      "34/34 [==============================] - 667s 20s/step - loss: 1.1439 - categorical_accuracy: 0.5452 - val_loss: 2.2288 - val_categorical_accuracy: 0.1400 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.9877 - categorical_accuracy: 0.6199 \n",
      "Epoch 00003: saving model to model_init_2022-01-2503_50_29.368234/model-00003-0.98774-0.61991-3.02967-0.16000.h5\n",
      "34/34 [==============================] - 673s 20s/step - loss: 0.9877 - categorical_accuracy: 0.6199 - val_loss: 3.0297 - val_categorical_accuracy: 0.1600 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.8263 - categorical_accuracy: 0.6931 \n",
      "Epoch 00004: saving model to model_init_2022-01-2503_50_29.368234/model-00004-0.82626-0.69306-2.98946-0.18000.h5\n",
      "34/34 [==============================] - 669s 20s/step - loss: 0.8263 - categorical_accuracy: 0.6931 - val_loss: 2.9895 - val_categorical_accuracy: 0.1800 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.7732 - categorical_accuracy: 0.7051 \n",
      "Epoch 00005: saving model to model_init_2022-01-2503_50_29.368234/model-00005-0.77319-0.70513-3.14698-0.21000.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "34/34 [==============================] - 664s 20s/step - loss: 0.7732 - categorical_accuracy: 0.7051 - val_loss: 3.1470 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5751 - categorical_accuracy: 0.7941 \n",
      "Epoch 00006: saving model to model_init_2022-01-2503_50_29.368234/model-00006-0.57512-0.79412-3.24206-0.16000.h5\n",
      "34/34 [==============================] - 678s 20s/step - loss: 0.5751 - categorical_accuracy: 0.7941 - val_loss: 3.2421 - val_categorical_accuracy: 0.1600 - lr: 2.0000e-04\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4494 - categorical_accuracy: 0.8356 \n",
      "Epoch 00007: saving model to model_init_2022-01-2503_50_29.368234/model-00007-0.44939-0.83560-3.24998-0.25000.h5\n",
      "34/34 [==============================] - 677s 20s/step - loss: 0.4494 - categorical_accuracy: 0.8356 - val_loss: 3.2500 - val_categorical_accuracy: 0.2500 - lr: 2.0000e-04\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4017 - categorical_accuracy: 0.8635 \n",
      "Epoch 00008: saving model to model_init_2022-01-2503_50_29.368234/model-00008-0.40172-0.86350-2.97022-0.19000.h5\n",
      "34/34 [==============================] - 673s 20s/step - loss: 0.4017 - categorical_accuracy: 0.8635 - val_loss: 2.9702 - val_categorical_accuracy: 0.1900 - lr: 2.0000e-04\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3550 - categorical_accuracy: 0.8680 \n",
      "Epoch 00009: saving model to model_init_2022-01-2503_50_29.368234/model-00009-0.35496-0.86802-3.07454-0.31000.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "34/34 [==============================] - 681s 20s/step - loss: 0.3550 - categorical_accuracy: 0.8680 - val_loss: 3.0745 - val_categorical_accuracy: 0.3100 - lr: 2.0000e-04\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2990 - categorical_accuracy: 0.9080 \n",
      "Epoch 00010: saving model to model_init_2022-01-2503_50_29.368234/model-00010-0.29902-0.90799-2.85363-0.35000.h5\n",
      "34/34 [==============================] - 644s 19s/step - loss: 0.2990 - categorical_accuracy: 0.9080 - val_loss: 2.8536 - val_categorical_accuracy: 0.3500 - lr: 4.0000e-05\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2874 - categorical_accuracy: 0.9050 \n",
      "Epoch 00011: saving model to model_init_2022-01-2503_50_29.368234/model-00011-0.28745-0.90498-2.76348-0.33000.h5\n",
      "34/34 [==============================] - 636s 19s/step - loss: 0.2874 - categorical_accuracy: 0.9050 - val_loss: 2.7635 - val_categorical_accuracy: 0.3300 - lr: 4.0000e-05\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2828 - categorical_accuracy: 0.9057 \n",
      "Epoch 00012: saving model to model_init_2022-01-2503_50_29.368234/model-00012-0.28276-0.90573-2.21771-0.45000.h5\n",
      "34/34 [==============================] - 657s 19s/step - loss: 0.2828 - categorical_accuracy: 0.9057 - val_loss: 2.2177 - val_categorical_accuracy: 0.4500 - lr: 4.0000e-05\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2821 - categorical_accuracy: 0.9118 \n",
      "Epoch 00013: saving model to model_init_2022-01-2503_50_29.368234/model-00013-0.28206-0.91176-2.17571-0.41000.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "34/34 [==============================] - 673s 20s/step - loss: 0.2821 - categorical_accuracy: 0.9118 - val_loss: 2.1757 - val_categorical_accuracy: 0.4100 - lr: 4.0000e-05\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2632 - categorical_accuracy: 0.9268 \n",
      "Epoch 00014: saving model to model_init_2022-01-2503_50_29.368234/model-00014-0.26317-0.92685-1.63414-0.53000.h5\n",
      "34/34 [==============================] - 673s 20s/step - loss: 0.2632 - categorical_accuracy: 0.9268 - val_loss: 1.6341 - val_categorical_accuracy: 0.5300 - lr: 8.0000e-06\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2657 - categorical_accuracy: 0.9261 \n",
      "Epoch 00015: saving model to model_init_2022-01-2503_50_29.368234/model-00015-0.26574-0.92609-1.42227-0.56000.h5\n",
      "34/34 [==============================] - 638s 19s/step - loss: 0.2657 - categorical_accuracy: 0.9261 - val_loss: 1.4223 - val_categorical_accuracy: 0.5600 - lr: 8.0000e-06\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2684 - categorical_accuracy: 0.9238 \n",
      "Epoch 00016: saving model to model_init_2022-01-2503_50_29.368234/model-00016-0.26836-0.92383-1.13134-0.60000.h5\n",
      "34/34 [==============================] - 640s 19s/step - loss: 0.2684 - categorical_accuracy: 0.9238 - val_loss: 1.1313 - val_categorical_accuracy: 0.6000 - lr: 8.0000e-06\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2730 - categorical_accuracy: 0.9170 \n",
      "Epoch 00017: saving model to model_init_2022-01-2503_50_29.368234/model-00017-0.27302-0.91704-0.95391-0.64000.h5\n",
      "34/34 [==============================] - 638s 19s/step - loss: 0.2730 - categorical_accuracy: 0.9170 - val_loss: 0.9539 - val_categorical_accuracy: 0.6400 - lr: 8.0000e-06\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2588 - categorical_accuracy: 0.9246 \n",
      "Epoch 00018: saving model to model_init_2022-01-2503_50_29.368234/model-00018-0.25885-0.92459-0.80321-0.68000.h5\n",
      "34/34 [==============================] - 651s 19s/step - loss: 0.2588 - categorical_accuracy: 0.9246 - val_loss: 0.8032 - val_categorical_accuracy: 0.6800 - lr: 8.0000e-06\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2352 - categorical_accuracy: 0.9314 \n",
      "Epoch 00019: saving model to model_init_2022-01-2503_50_29.368234/model-00019-0.23522-0.93137-0.86177-0.69000.h5\n",
      "34/34 [==============================] - 723s 21s/step - loss: 0.2352 - categorical_accuracy: 0.9314 - val_loss: 0.8618 - val_categorical_accuracy: 0.6900 - lr: 8.0000e-06\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2557 - categorical_accuracy: 0.9238 \n",
      "Epoch 00020: saving model to model_init_2022-01-2503_50_29.368234/model-00020-0.25570-0.92383-0.69605-0.69000.h5\n",
      "34/34 [==============================] - 712s 21s/step - loss: 0.2557 - categorical_accuracy: 0.9238 - val_loss: 0.6960 - val_categorical_accuracy: 0.6900 - lr: 8.0000e-06\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Params:\", rnn_cnn1_model.count_params())\n",
    "history_model9=rnn_cnn1.train_model(rnn_cnn1_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMBYtFWLogi6"
   },
   "source": [
    "#### OBSERVATION:\n",
    "Model is overfitting, let’s try adding more augmentation and reducing layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24RrDW-L3ZpZ"
   },
   "source": [
    "Model 6 - CNN - RNN 2 [GRU]\n",
    "\n",
    "Adding more augmentation and reducing layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yC_bjKBU3Xq2"
   },
   "outputs": [],
   "source": [
    "class RNNCNN2(ModelBuilderMoreAugmentation):\n",
    "    \n",
    "    def define_model(self,lstm_cells=64,dense_neurons=64,dropout=0.25):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),\n",
    "                                  input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "        model.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "        model.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "        model.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "\n",
    "        model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "        model.add(GRU(lstm_cells))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "        # optimiser = optimizers.Adam(lr=0.0002)\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ymbjMzQu4nE0",
    "outputId": "abd37b23-d985-4686-fb35-3e0e124d727e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_21 (TimeDi  (None, 18, 120, 120, 16)  448      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_22 (TimeDi  (None, 18, 120, 120, 16)  64       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_23 (TimeDi  (None, 18, 60, 60, 16)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_24 (TimeDi  (None, 18, 60, 60, 32)   4640      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_25 (TimeDi  (None, 18, 60, 60, 32)   128       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_26 (TimeDi  (None, 18, 30, 30, 32)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_27 (TimeDi  (None, 18, 30, 30, 64)   18496     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_28 (TimeDi  (None, 18, 30, 30, 64)   256       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_29 (TimeDi  (None, 18, 15, 15, 64)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_30 (TimeDi  (None, 18, 15, 15, 128)  73856     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_31 (TimeDi  (None, 18, 15, 15, 128)  512       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_32 (TimeDi  (None, 18, 7, 7, 128)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_33 (TimeDi  (None, 18, 6272)         0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " gru_3 (GRU)                 (None, 128)               2458368   \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,573,925\n",
      "Trainable params: 2,573,445\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_cnn2=RNNCNN2()\n",
    "rnn_cnn2.initialize_path(project_folder)\n",
    "rnn_cnn2.initialize_image_properties(image_height=120,image_width=120)\n",
    "rnn_cnn2.initialize_hyperparams(frames_to_sample=18,batch_size=20,num_epochs=20)\n",
    "rnn_cnn2_model=rnn_cnn2.define_model(lstm_cells=128,dense_neurons=128,dropout=0.25)\n",
    "rnn_cnn2_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OpClb_P44nZy",
    "outputId": "b10371fe-410b-47e9-f462-7588b3b9bf27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 2573925\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:130: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: DeprecationWarning:     `imread` is deprecated!\n",
      "    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``imageio.imread`` instead.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:56: DeprecationWarning:     `imresize` is deprecated!\n",
      "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``skimage.transform.resize`` instead.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:75: DeprecationWarning:     `imresize` is deprecated!\n",
      "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.5233 - categorical_accuracy: 0.3552 \n",
      "Epoch 00001: val_loss improved from inf to 3.04009, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2515_38_43.699099/model-00001-1.52328-0.35520-3.04009-0.18000.h5\n",
      "34/34 [==============================] - 1425s 43s/step - loss: 1.5233 - categorical_accuracy: 0.3552 - val_loss: 3.0401 - val_categorical_accuracy: 0.1800 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.1397 - categorical_accuracy: 0.5189\n",
      "Epoch 00002: val_loss improved from 3.04009 to 2.53401, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2515_38_43.699099/model-00002-1.13971-0.51885-2.53401-0.20000.h5\n",
      "34/34 [==============================] - 139s 4s/step - loss: 1.1397 - categorical_accuracy: 0.5189 - val_loss: 2.5340 - val_categorical_accuracy: 0.2000 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.9521 - categorical_accuracy: 0.6131\n",
      "Epoch 00003: val_loss improved from 2.53401 to 2.26289, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2515_38_43.699099/model-00003-0.95210-0.61312-2.26289-0.22000.h5\n",
      "34/34 [==============================] - 144s 4s/step - loss: 0.9521 - categorical_accuracy: 0.6131 - val_loss: 2.2629 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.8205 - categorical_accuracy: 0.6697\n",
      "Epoch 00004: val_loss did not improve from 2.26289\n",
      "34/34 [==============================] - 137s 4s/step - loss: 0.8205 - categorical_accuracy: 0.6697 - val_loss: 2.3503 - val_categorical_accuracy: 0.1700 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.7345 - categorical_accuracy: 0.7157\n",
      "Epoch 00005: val_loss did not improve from 2.26289\n",
      "34/34 [==============================] - 143s 4s/step - loss: 0.7345 - categorical_accuracy: 0.7157 - val_loss: 2.5090 - val_categorical_accuracy: 0.1800 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.6423 - categorical_accuracy: 0.7564\n",
      "Epoch 00006: val_loss did not improve from 2.26289\n",
      "34/34 [==============================] - 138s 4s/step - loss: 0.6423 - categorical_accuracy: 0.7564 - val_loss: 2.6469 - val_categorical_accuracy: 0.2600 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.6141 - categorical_accuracy: 0.7707\n",
      "Epoch 00007: val_loss improved from 2.26289 to 2.20816, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2515_38_43.699099/model-00007-0.61412-0.77074-2.20816-0.23000.h5\n",
      "34/34 [==============================] - 138s 4s/step - loss: 0.6141 - categorical_accuracy: 0.7707 - val_loss: 2.2082 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4770 - categorical_accuracy: 0.8220\n",
      "Epoch 00008: val_loss did not improve from 2.20816\n",
      "34/34 [==============================] - 138s 4s/step - loss: 0.4770 - categorical_accuracy: 0.8220 - val_loss: 2.2629 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4106 - categorical_accuracy: 0.8492\n",
      "Epoch 00009: val_loss did not improve from 2.20816\n",
      "34/34 [==============================] - 137s 4s/step - loss: 0.4106 - categorical_accuracy: 0.8492 - val_loss: 2.6776 - val_categorical_accuracy: 0.2800 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.6178 - categorical_accuracy: 0.7700\n",
      "Epoch 00010: val_loss did not improve from 2.20816\n",
      "34/34 [==============================] - 138s 4s/step - loss: 0.6178 - categorical_accuracy: 0.7700 - val_loss: 4.3137 - val_categorical_accuracy: 0.1900 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3920 - categorical_accuracy: 0.8567\n",
      "Epoch 00011: val_loss did not improve from 2.20816\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "34/34 [==============================] - 136s 4s/step - loss: 0.3920 - categorical_accuracy: 0.8567 - val_loss: 2.8556 - val_categorical_accuracy: 0.2900 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3029 - categorical_accuracy: 0.8944\n",
      "Epoch 00012: val_loss did not improve from 2.20816\n",
      "34/34 [==============================] - 140s 4s/step - loss: 0.3029 - categorical_accuracy: 0.8944 - val_loss: 3.2462 - val_categorical_accuracy: 0.2500 - lr: 2.0000e-04\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2542 - categorical_accuracy: 0.9155\n",
      "Epoch 00013: val_loss did not improve from 2.20816\n",
      "34/34 [==============================] - 138s 4s/step - loss: 0.2542 - categorical_accuracy: 0.9155 - val_loss: 2.6462 - val_categorical_accuracy: 0.3600 - lr: 2.0000e-04\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2057 - categorical_accuracy: 0.9291\n",
      "Epoch 00014: val_loss did not improve from 2.20816\n",
      "34/34 [==============================] - 137s 4s/step - loss: 0.2057 - categorical_accuracy: 0.9291 - val_loss: 2.3898 - val_categorical_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1782 - categorical_accuracy: 0.9389\n",
      "Epoch 00015: val_loss improved from 2.20816 to 1.35724, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2515_38_43.699099/model-00015-0.17819-0.93891-1.35724-0.60000.h5\n",
      "34/34 [==============================] - 138s 4s/step - loss: 0.1782 - categorical_accuracy: 0.9389 - val_loss: 1.3572 - val_categorical_accuracy: 0.6000 - lr: 2.0000e-04\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1604 - categorical_accuracy: 0.9525\n",
      "Epoch 00016: val_loss improved from 1.35724 to 1.23626, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2515_38_43.699099/model-00016-0.16044-0.95249-1.23626-0.60000.h5\n",
      "34/34 [==============================] - 138s 4s/step - loss: 0.1604 - categorical_accuracy: 0.9525 - val_loss: 1.2363 - val_categorical_accuracy: 0.6000 - lr: 2.0000e-04\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1453 - categorical_accuracy: 0.9615\n",
      "Epoch 00017: val_loss improved from 1.23626 to 1.11402, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2515_38_43.699099/model-00017-0.14532-0.96154-1.11402-0.65000.h5\n",
      "34/34 [==============================] - 139s 4s/step - loss: 0.1453 - categorical_accuracy: 0.9615 - val_loss: 1.1140 - val_categorical_accuracy: 0.6500 - lr: 2.0000e-04\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1302 - categorical_accuracy: 0.9630\n",
      "Epoch 00018: val_loss improved from 1.11402 to 0.64980, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2515_38_43.699099/model-00018-0.13022-0.96305-0.64980-0.77000.h5\n",
      "34/34 [==============================] - 139s 4s/step - loss: 0.1302 - categorical_accuracy: 0.9630 - val_loss: 0.6498 - val_categorical_accuracy: 0.7700 - lr: 2.0000e-04\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1427 - categorical_accuracy: 0.9615\n",
      "Epoch 00019: val_loss improved from 0.64980 to 0.59553, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2515_38_43.699099/model-00019-0.14268-0.96154-0.59553-0.76000.h5\n",
      "34/34 [==============================] - 139s 4s/step - loss: 0.1427 - categorical_accuracy: 0.9615 - val_loss: 0.5955 - val_categorical_accuracy: 0.7600 - lr: 2.0000e-04\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1286 - categorical_accuracy: 0.9585\n",
      "Epoch 00020: val_loss improved from 0.59553 to 0.53718, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2515_38_43.699099/model-00020-0.12861-0.95852-0.53718-0.78000.h5\n",
      "34/34 [==============================] - 138s 4s/step - loss: 0.1286 - categorical_accuracy: 0.9585 - val_loss: 0.5372 - val_categorical_accuracy: 0.7800 - lr: 2.0000e-04\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Params:\", rnn_cnn2_model.count_params())\n",
    "history_model17=rnn_cnn2.train_model(rnn_cnn2_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvQBQk1No8kq"
   },
   "source": [
    "#### OBSERVATION:\n",
    "More augmentation reduced overfitting, to further improve accuracy, let’s use transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBAMf9hNW1xx"
   },
   "source": [
    "Model 7 - Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZuyQbZqW2JI"
   },
   "outputs": [],
   "source": [
    "from keras.applications import mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HgZKyteOXdOd",
    "outputId": "576af248-a2cd-46c8-b80c-2840e47aadd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "mobilenet_transfer = mobilenet.MobileNet(weights='imagenet', include_top=False)\n",
    "\n",
    "class RNNCNN_TL(ModelBuilderMoreAugmentation):\n",
    "    \n",
    "    def define_model(self,lstm_cells=64,dense_neurons=64,dropout=0.25):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(TimeDistributed(mobilenet_transfer,input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    "        \n",
    "        \n",
    "        for layer in model.layers:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        \n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "        model.add(LSTM(lstm_cells))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "        \n",
    "        \n",
    "        # optimiser = optimizers.Adam()\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DCzv8NCTXmtd",
    "outputId": "71f9f378-2f7a-41cf-bad6-d9fd80a91bc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_36 (TimeDi  (None, 16, 3, 3, 1024)   3228864   \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_37 (TimeDi  (None, 16, 3, 3, 1024)   4096      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_38 (TimeDi  (None, 16, 1, 1, 1024)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_39 (TimeDi  (None, 16, 1024)         0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 128)               590336    \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,840,453\n",
      "Trainable params: 609,541\n",
      "Non-trainable params: 3,230,912\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_cnn_tl=RNNCNN_TL()\n",
    "rnn_cnn_tl.initialize_path(project_folder)\n",
    "rnn_cnn_tl.initialize_image_properties(image_height=120,image_width=120)\n",
    "rnn_cnn_tl.initialize_hyperparams(frames_to_sample=16,batch_size=5,num_epochs=20)\n",
    "rnn_cnn_tl_model=rnn_cnn_tl.define_model(lstm_cells=128,dense_neurons=128,dropout=0.25)\n",
    "rnn_cnn_tl_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CWYI5SYnXrId",
    "outputId": "1f0f358e-9353-4f05-c279-1bc0543d60df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 3840453\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:128: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: DeprecationWarning:     `imread` is deprecated!\n",
      "    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``imageio.imread`` instead.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:56: DeprecationWarning:     `imresize` is deprecated!\n",
      "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``skimage.transform.resize`` instead.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:75: DeprecationWarning:     `imresize` is deprecated!\n",
      "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.3158 - categorical_accuracy: 0.4540\n",
      "Epoch 00001: saving model to model_init_2022-01-2508_24_14.943310/model-00001-1.31585-0.45400-0.93543-0.70000.h5\n",
      "133/133 [==============================] - 1196s 9s/step - loss: 1.3158 - categorical_accuracy: 0.4540 - val_loss: 0.9354 - val_categorical_accuracy: 0.7000 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9045 - categorical_accuracy: 0.6365\n",
      "Epoch 00002: saving model to model_init_2022-01-2508_24_14.943310/model-00002-0.90447-0.63650-0.70300-0.77000.h5\n",
      "133/133 [==============================] - 311s 2s/step - loss: 0.9045 - categorical_accuracy: 0.6365 - val_loss: 0.7030 - val_categorical_accuracy: 0.7700 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6660 - categorical_accuracy: 0.7353\n",
      "Epoch 00003: saving model to model_init_2022-01-2508_24_14.943310/model-00003-0.66601-0.73529-0.74815-0.71000.h5\n",
      "133/133 [==============================] - 306s 2s/step - loss: 0.6660 - categorical_accuracy: 0.7353 - val_loss: 0.7481 - val_categorical_accuracy: 0.7100 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5032 - categorical_accuracy: 0.8281\n",
      "Epoch 00004: saving model to model_init_2022-01-2508_24_14.943310/model-00004-0.50318-0.82805-0.53194-0.86000.h5\n",
      "133/133 [==============================] - 308s 2s/step - loss: 0.5032 - categorical_accuracy: 0.8281 - val_loss: 0.5319 - val_categorical_accuracy: 0.8600 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3702 - categorical_accuracy: 0.8688\n",
      "Epoch 00005: saving model to model_init_2022-01-2508_24_14.943310/model-00005-0.37025-0.86878-0.71112-0.71000.h5\n",
      "133/133 [==============================] - 308s 2s/step - loss: 0.3702 - categorical_accuracy: 0.8688 - val_loss: 0.7111 - val_categorical_accuracy: 0.7100 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3493 - categorical_accuracy: 0.8733\n",
      "Epoch 00006: saving model to model_init_2022-01-2508_24_14.943310/model-00006-0.34926-0.87330-0.47867-0.82000.h5\n",
      "133/133 [==============================] - 309s 2s/step - loss: 0.3493 - categorical_accuracy: 0.8733 - val_loss: 0.4787 - val_categorical_accuracy: 0.8200 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3032 - categorical_accuracy: 0.8824\n",
      "Epoch 00007: saving model to model_init_2022-01-2508_24_14.943310/model-00007-0.30323-0.88235-0.68112-0.75000.h5\n",
      "133/133 [==============================] - 310s 2s/step - loss: 0.3032 - categorical_accuracy: 0.8824 - val_loss: 0.6811 - val_categorical_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.2167 - categorical_accuracy: 0.9238\n",
      "Epoch 00008: saving model to model_init_2022-01-2508_24_14.943310/model-00008-0.21675-0.92383-0.51162-0.80000.h5\n",
      "133/133 [==============================] - 307s 2s/step - loss: 0.2167 - categorical_accuracy: 0.9238 - val_loss: 0.5116 - val_categorical_accuracy: 0.8000 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.2163 - categorical_accuracy: 0.9291\n",
      "Epoch 00009: saving model to model_init_2022-01-2508_24_14.943310/model-00009-0.21627-0.92911-0.85990-0.71000.h5\n",
      "133/133 [==============================] - 314s 2s/step - loss: 0.2163 - categorical_accuracy: 0.9291 - val_loss: 0.8599 - val_categorical_accuracy: 0.7100 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.1481 - categorical_accuracy: 0.9472\n",
      "Epoch 00010: saving model to model_init_2022-01-2508_24_14.943310/model-00010-0.14810-0.94721-0.63608-0.80000.h5\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "133/133 [==============================] - 308s 2s/step - loss: 0.1481 - categorical_accuracy: 0.9472 - val_loss: 0.6361 - val_categorical_accuracy: 0.8000 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.1195 - categorical_accuracy: 0.9623\n",
      "Epoch 00011: saving model to model_init_2022-01-2508_24_14.943310/model-00011-0.11948-0.96229-0.76505-0.78000.h5\n",
      "133/133 [==============================] - 304s 2s/step - loss: 0.1195 - categorical_accuracy: 0.9623 - val_loss: 0.7651 - val_categorical_accuracy: 0.7800 - lr: 2.0000e-04\n",
      "Epoch 12/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0944 - categorical_accuracy: 0.9721\n",
      "Epoch 00012: saving model to model_init_2022-01-2508_24_14.943310/model-00012-0.09440-0.97210-0.59063-0.81000.h5\n",
      "133/133 [==============================] - 325s 2s/step - loss: 0.0944 - categorical_accuracy: 0.9721 - val_loss: 0.5906 - val_categorical_accuracy: 0.8100 - lr: 2.0000e-04\n",
      "Epoch 13/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0918 - categorical_accuracy: 0.9623\n",
      "Epoch 00013: saving model to model_init_2022-01-2508_24_14.943310/model-00013-0.09185-0.96229-0.69879-0.79000.h5\n",
      "133/133 [==============================] - 309s 2s/step - loss: 0.0918 - categorical_accuracy: 0.9623 - val_loss: 0.6988 - val_categorical_accuracy: 0.7900 - lr: 2.0000e-04\n",
      "Epoch 14/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0878 - categorical_accuracy: 0.9691\n",
      "Epoch 00014: saving model to model_init_2022-01-2508_24_14.943310/model-00014-0.08777-0.96908-0.65342-0.81000.h5\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "133/133 [==============================] - 308s 2s/step - loss: 0.0878 - categorical_accuracy: 0.9691 - val_loss: 0.6534 - val_categorical_accuracy: 0.8100 - lr: 2.0000e-04\n",
      "Epoch 15/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0636 - categorical_accuracy: 0.9759\n",
      "Epoch 00015: saving model to model_init_2022-01-2508_24_14.943310/model-00015-0.06364-0.97587-0.57570-0.80000.h5\n",
      "133/133 [==============================] - 311s 2s/step - loss: 0.0636 - categorical_accuracy: 0.9759 - val_loss: 0.5757 - val_categorical_accuracy: 0.8000 - lr: 4.0000e-05\n",
      "Epoch 16/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0972 - categorical_accuracy: 0.9615\n",
      "Epoch 00016: saving model to model_init_2022-01-2508_24_14.943310/model-00016-0.09722-0.96154-0.60804-0.80000.h5\n",
      "133/133 [==============================] - 310s 2s/step - loss: 0.0972 - categorical_accuracy: 0.9615 - val_loss: 0.6080 - val_categorical_accuracy: 0.8000 - lr: 4.0000e-05\n",
      "Epoch 17/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0695 - categorical_accuracy: 0.9713\n",
      "Epoch 00017: saving model to model_init_2022-01-2508_24_14.943310/model-00017-0.06947-0.97134-0.59930-0.79000.h5\n",
      "133/133 [==============================] - 306s 2s/step - loss: 0.0695 - categorical_accuracy: 0.9713 - val_loss: 0.5993 - val_categorical_accuracy: 0.7900 - lr: 4.0000e-05\n",
      "Epoch 18/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0581 - categorical_accuracy: 0.9811\n",
      "Epoch 00018: saving model to model_init_2022-01-2508_24_14.943310/model-00018-0.05806-0.98115-0.58756-0.82000.h5\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "133/133 [==============================] - 309s 2s/step - loss: 0.0581 - categorical_accuracy: 0.9811 - val_loss: 0.5876 - val_categorical_accuracy: 0.8200 - lr: 4.0000e-05\n",
      "Epoch 19/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0628 - categorical_accuracy: 0.9796\n",
      "Epoch 00019: saving model to model_init_2022-01-2508_24_14.943310/model-00019-0.06285-0.97964-0.66584-0.78000.h5\n",
      "133/133 [==============================] - 305s 2s/step - loss: 0.0628 - categorical_accuracy: 0.9796 - val_loss: 0.6658 - val_categorical_accuracy: 0.7800 - lr: 8.0000e-06\n",
      "Epoch 20/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0711 - categorical_accuracy: 0.9781\n",
      "Epoch 00020: saving model to model_init_2022-01-2508_24_14.943310/model-00020-0.07111-0.97813-0.60004-0.81000.h5\n",
      "133/133 [==============================] - 309s 2s/step - loss: 0.0711 - categorical_accuracy: 0.9781 - val_loss: 0.6000 - val_categorical_accuracy: 0.8100 - lr: 8.0000e-06\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Params:\", rnn_cnn_tl_model.count_params())\n",
    "history_model18=rnn_cnn_tl.train_model(rnn_cnn_tl_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PrhSJGTpLAn"
   },
   "source": [
    "#### OBSERVATION:\n",
    "Validation accuracy is poor as mobilenet weights not trained. Let’s train them also.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eZ9IjcFXynk"
   },
   "source": [
    "Model 8 - Transfer Learning with GRU and training all weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oqXoBFbJX5xM",
    "outputId": "b1e21aac-cbb2-46c4-cd26-10cc44f9a246"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n",
      "17227776/17225924 [==============================] - 0s 0us/step\n",
      "17235968/17225924 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import mobilenet\n",
    "\n",
    "mobilenet_transfer = mobilenet.MobileNet(weights='imagenet', include_top=False)\n",
    "\n",
    "class RNNCNN_TL2(ModelBuilderMoreAugmentation):\n",
    "    \n",
    "    def define_model(self,gru_cells=64,dense_neurons=64,dropout=0.25):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(TimeDistributed(mobilenet_transfer,input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
    " \n",
    "        \n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "        model.add(GRU(gru_cells))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        model.add(Dense(dense_neurons,activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "        \n",
    "        \n",
    "        # optimiser = optimizers.Adam()\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Zrlj56qYEVy",
    "outputId": "7f86e01f-6fbc-4371-fea7-e13378876484"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_4 (TimeDis  (None, 16, 3, 3, 1024)   3228864   \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_5 (TimeDis  (None, 16, 3, 3, 1024)   4096      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_6 (TimeDis  (None, 16, 1, 1, 1024)   0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_7 (TimeDis  (None, 16, 1024)         0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 128)               443136    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,693,253\n",
      "Trainable params: 3,669,317\n",
      "Non-trainable params: 23,936\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_cnn_tl2=RNNCNN_TL2()\n",
    "rnn_cnn_tl2.initialize_path(project_folder)\n",
    "rnn_cnn_tl2.initialize_image_properties(image_height=120,image_width=120)\n",
    "rnn_cnn_tl2.initialize_hyperparams(frames_to_sample=16,batch_size=5,num_epochs=20)\n",
    "rnn_cnn_tl2_model=rnn_cnn_tl2.define_model(gru_cells=128,dense_neurons=128,dropout=0.25)\n",
    "rnn_cnn_tl2_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xnd7e3HAYN5r",
    "outputId": "1e82df38-9884-47b1-c192-907518a4d252"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 3693253\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:130: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: DeprecationWarning:     `imread` is deprecated!\n",
      "    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``imageio.imread`` instead.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:56: DeprecationWarning:     `imresize` is deprecated!\n",
      "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``skimage.transform.resize`` instead.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:75: DeprecationWarning:     `imresize` is deprecated!\n",
      "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.0949 - categorical_accuracy: 0.5437\n",
      "Epoch 00001: val_loss improved from inf to 0.64710, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2514_39_58.290277/model-00001-1.09493-0.54374-0.64710-0.75000.h5\n",
      "133/133 [==============================] - 144s 1s/step - loss: 1.0949 - categorical_accuracy: 0.5437 - val_loss: 0.6471 - val_categorical_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6162 - categorical_accuracy: 0.7647\n",
      "Epoch 00002: val_loss improved from 0.64710 to 0.41340, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2514_39_58.290277/model-00002-0.61621-0.76471-0.41340-0.79000.h5\n",
      "133/133 [==============================] - 137s 1s/step - loss: 0.6162 - categorical_accuracy: 0.7647 - val_loss: 0.4134 - val_categorical_accuracy: 0.7900 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4798 - categorical_accuracy: 0.8243\n",
      "Epoch 00003: val_loss did not improve from 0.41340\n",
      "133/133 [==============================] - 137s 1s/step - loss: 0.4798 - categorical_accuracy: 0.8243 - val_loss: 0.7549 - val_categorical_accuracy: 0.7400 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4510 - categorical_accuracy: 0.8363\n",
      "Epoch 00004: val_loss improved from 0.41340 to 0.35659, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2514_39_58.290277/model-00004-0.45100-0.83635-0.35659-0.90000.h5\n",
      "133/133 [==============================] - 137s 1s/step - loss: 0.4510 - categorical_accuracy: 0.8363 - val_loss: 0.3566 - val_categorical_accuracy: 0.9000 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.2803 - categorical_accuracy: 0.8937\n",
      "Epoch 00005: val_loss did not improve from 0.35659\n",
      "133/133 [==============================] - 136s 1s/step - loss: 0.2803 - categorical_accuracy: 0.8937 - val_loss: 0.3758 - val_categorical_accuracy: 0.8400 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.2557 - categorical_accuracy: 0.9155\n",
      "Epoch 00006: val_loss improved from 0.35659 to 0.29063, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2514_39_58.290277/model-00006-0.25571-0.91554-0.29063-0.87000.h5\n",
      "133/133 [==============================] - 136s 1s/step - loss: 0.2557 - categorical_accuracy: 0.9155 - val_loss: 0.2906 - val_categorical_accuracy: 0.8700 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3016 - categorical_accuracy: 0.8997\n",
      "Epoch 00007: val_loss did not improve from 0.29063\n",
      "133/133 [==============================] - 136s 1s/step - loss: 0.3016 - categorical_accuracy: 0.8997 - val_loss: 0.4201 - val_categorical_accuracy: 0.8000 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3150 - categorical_accuracy: 0.8989\n",
      "Epoch 00008: val_loss did not improve from 0.29063\n",
      "133/133 [==============================] - 135s 1s/step - loss: 0.3150 - categorical_accuracy: 0.8989 - val_loss: 0.4427 - val_categorical_accuracy: 0.8900 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.2519 - categorical_accuracy: 0.9005\n",
      "Epoch 00009: val_loss did not improve from 0.29063\n",
      "133/133 [==============================] - 135s 1s/step - loss: 0.2519 - categorical_accuracy: 0.9005 - val_loss: 0.5494 - val_categorical_accuracy: 0.8100 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.2858 - categorical_accuracy: 0.8929\n",
      "Epoch 00010: val_loss improved from 0.29063 to 0.24217, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2514_39_58.290277/model-00010-0.28579-0.89291-0.24217-0.94000.h5\n",
      "133/133 [==============================] - 138s 1s/step - loss: 0.2858 - categorical_accuracy: 0.8929 - val_loss: 0.2422 - val_categorical_accuracy: 0.9400 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.2061 - categorical_accuracy: 0.9291\n",
      "Epoch 00011: val_loss improved from 0.24217 to 0.24048, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2514_39_58.290277/model-00011-0.20607-0.92911-0.24048-0.89000.h5\n",
      "133/133 [==============================] - 136s 1s/step - loss: 0.2061 - categorical_accuracy: 0.9291 - val_loss: 0.2405 - val_categorical_accuracy: 0.8900 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.1804 - categorical_accuracy: 0.9359\n",
      "Epoch 00012: val_loss improved from 0.24048 to 0.19649, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2514_39_58.290277/model-00012-0.18042-0.93590-0.19649-0.92000.h5\n",
      "133/133 [==============================] - 137s 1s/step - loss: 0.1804 - categorical_accuracy: 0.9359 - val_loss: 0.1965 - val_categorical_accuracy: 0.9200 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.1432 - categorical_accuracy: 0.9487\n",
      "Epoch 00013: val_loss did not improve from 0.19649\n",
      "133/133 [==============================] - 137s 1s/step - loss: 0.1432 - categorical_accuracy: 0.9487 - val_loss: 0.4031 - val_categorical_accuracy: 0.8400 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.2579 - categorical_accuracy: 0.9125\n",
      "Epoch 00014: val_loss improved from 0.19649 to 0.10952, saving model to /content/gdrive/MyDrive/app/Gesture_Recognition_Assignment/Geture_h5 File/model_init_2022-01-2514_39_58.290277/model-00014-0.25789-0.91252-0.10952-0.98000.h5\n",
      "133/133 [==============================] - 137s 1s/step - loss: 0.2579 - categorical_accuracy: 0.9125 - val_loss: 0.1095 - val_categorical_accuracy: 0.9800 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.2053 - categorical_accuracy: 0.9419\n",
      "Epoch 00015: val_loss did not improve from 0.10952\n",
      "133/133 [==============================] - 136s 1s/step - loss: 0.2053 - categorical_accuracy: 0.9419 - val_loss: 0.8919 - val_categorical_accuracy: 0.7600 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.1861 - categorical_accuracy: 0.9382\n",
      "Epoch 00016: val_loss did not improve from 0.10952\n",
      "133/133 [==============================] - 136s 1s/step - loss: 0.1861 - categorical_accuracy: 0.9382 - val_loss: 0.3974 - val_categorical_accuracy: 0.8500 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.1956 - categorical_accuracy: 0.9495\n",
      "Epoch 00017: val_loss did not improve from 0.10952\n",
      "133/133 [==============================] - 136s 1s/step - loss: 0.1956 - categorical_accuracy: 0.9495 - val_loss: 0.6054 - val_categorical_accuracy: 0.8000 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.2770 - categorical_accuracy: 0.9118\n",
      "Epoch 00018: val_loss did not improve from 0.10952\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "133/133 [==============================] - 136s 1s/step - loss: 0.2770 - categorical_accuracy: 0.9118 - val_loss: 0.4805 - val_categorical_accuracy: 0.8200 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.1187 - categorical_accuracy: 0.9623\n",
      "Epoch 00019: val_loss did not improve from 0.10952\n",
      "133/133 [==============================] - 136s 1s/step - loss: 0.1187 - categorical_accuracy: 0.9623 - val_loss: 0.1811 - val_categorical_accuracy: 0.9300 - lr: 2.0000e-04\n",
      "Epoch 20/20\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0603 - categorical_accuracy: 0.9827\n",
      "Epoch 00020: val_loss did not improve from 0.10952\n",
      "133/133 [==============================] - 135s 1s/step - loss: 0.0603 - categorical_accuracy: 0.9827 - val_loss: 0.1735 - val_categorical_accuracy: 0.9400 - lr: 2.0000e-04\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Params:\", rnn_cnn_tl2_model.count_params())\n",
    "history_model19=rnn_cnn_tl2.train_model(rnn_cnn_tl2_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oAu6iceupWbX"
   },
   "source": [
    "#### OBSERVATION:\n",
    "We get a better accuracy on training mobilenet layer’s weights as well.\n",
    "Hence, this is our final model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZChmXAf34kRZ"
   },
   "source": [
    "### LOADING BEST MODEL AND TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Xs_p3ZJJ4ssk"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from keras.models import load_model\n",
    "model = load_model('/content/gdrive/MyDrive/ML/model-00012-0.18042-0.93590-0.19649-0.92000.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HuOyIRx_46my",
    "outputId": "9aa60c99-c935-40fe-ee8e-aece7dbd6584"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: DeprecationWarning:     `imread` is deprecated!\n",
      "    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``imageio.imread`` instead.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:56: DeprecationWarning:     `imresize` is deprecated!\n",
      "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``skimage.transform.resize`` instead.\n"
     ]
    }
   ],
   "source": [
    "test_generator=RNNCNN_TL2()\n",
    "test_generator.initialize_path(project_folder)\n",
    "test_generator.initialize_image_properties(image_height=120,image_width=120)\n",
    "test_generator.initialize_hyperparams(frames_to_sample=16,batch_size=5,num_epochs=1)\n",
    "\n",
    "g=test_generator.generator(test_generator.val_path,test_generator.val_doc,augment=False)\n",
    "batch_data, batch_labels=next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PKTI_woF46yR",
    "outputId": "629f4481-43d8-4b18-8d41-415b77704b86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9cIg0XnY466W",
    "outputId": "f5f87f99-f413-4661-89e4-cbc5a64b85ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 4 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(model.predict(batch_data[:,:,:,:,:]),axis=1))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Neural_Nets_Project_Final1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
